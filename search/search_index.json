{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"The Internals of Spark Declarative Pipelines (Apache Spark 4.1.1)","text":"<p>Welcome to The Internals of Spark Declarative Pipelines online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, a Freelance Data(bricks) Engineer \ud83e\uddf1 specializing in Apache Spark (incl. Spark SQL and Spark Structured Streaming), Delta Lake, Unity Catalog, MLflow, DSPy, Databricks with brief forays into a wider Data, ML and AI Engineering space (mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Declarative Pipelines as much as I have.</p> <p>Flannery O'Connor</p> <p>I write to discover what I know.</p> <p>\"The Internals Of\" series</p> <p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p> <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Declarative Pipelines \ud83d\udd25</p> <p>Last update: 2026-01-24</p>"},{"location":"AppendOnceFlow/","title":"AppendOnceFlow","text":"<p><code>AppendOnceFlow</code> is a ResolvedFlow with once flag enabled.</p> <p><code>AppendOnceFlow</code> is created when CoreDataflowNodeProcessor is requested to process an UnresolvedFlow with once flag enabled (and FlowResolver is requested to convertResolvedToTypedFlow).</p> <p><code>AppendOnceFlow</code> reads source(s) completely and appends data to the target, just once.</p>"},{"location":"AppendOnceFlow/#creating-instance","title":"Creating Instance","text":"<p><code>AppendOnceFlow</code> takes the following to be created:</p> <ul> <li> UnresolvedFlow <li> FlowFunctionResult <p><code>AppendOnceFlow</code> is created when:</p> <ul> <li><code>FlowResolver</code> is requested to convertResolvedToTypedFlow (for an UnresolvedFlow with once flag enabled)</li> </ul>"},{"location":"AppendOnceFlow/#once","title":"once Flag","text":"Flow <pre><code>once: Boolean\n</code></pre> <p><code>once</code> is part of the Flow abstraction.</p> <p><code>once</code> is always enabled (<code>true</code>).</p>"},{"location":"BatchTableWrite/","title":"BatchTableWrite Flow Execution","text":"<p><code>BatchTableWrite</code> is a FlowExecution that represents a CompleteFlow at execution.</p> <p>When executed, <code>BatchTableWrite</code> writes a batch <code>DataFrame</code> (Spark SQL) to a Table destination.</p>"},{"location":"BatchTableWrite/#creating-instance","title":"Creating Instance","text":"<p><code>BatchTableWrite</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> ResolvedFlow <li> DataflowGraph <li> Table <li> PipelineUpdateContext <li> Configuration Properties <p><code>BatchTableWrite</code> is created when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a CompleteFlow</li> </ul>"},{"location":"BatchTableWrite/#executeInternal","title":"Execute","text":"FlowExecution <pre><code>executeInternal(): Future[Unit]\n</code></pre> <p><code>executeInternal</code> is part of the FlowExecution abstraction.</p> <p><code>executeInternal</code> activates the configuration properties in the current SparkSession.</p> <p><code>executeInternal</code> requests this PipelineUpdateContext for the FlowProgressEventLogger to recordRunning with this ResolvedFlow.</p> <p><code>executeInternal</code> requests this DataflowGraph to re-analyze this ResolvedFlow to get the DataFrame (the logical query plan)</p> <p><code>executeInternal</code> executes <code>append</code> batch write asynchronously:</p> <ol> <li>Creates a <code>DataFrameWriter</code> (Spark SQL) for the batch query's logical plan (the DataFrame).</li> <li>Sets the write format to the format of this Table.</li> <li>In the end, <code>executeInternal</code> appends the rows to this Table (using <code>DataFrameWriter.saveAsTable</code> (Spark SQL) operator).</li> </ol>"},{"location":"BatchTableWrite/#isStreaming","title":"isStreaming","text":"FlowExecution <pre><code>isStreaming: Boolean\n</code></pre> <p><code>isStreaming</code> is part of the FlowExecution abstraction.</p> <p><code>isStreaming</code> is always disabled (<code>false</code>).</p>"},{"location":"CompleteFlow/","title":"CompleteFlow","text":"<p><code>CompleteFlow</code> is a ResolvedFlow that may or may not be append.</p> <p><code>CompleteFlow</code> is planned for execution as BatchTableWrite by FlowPlanner.</p>"},{"location":"CompleteFlow/#creating-instance","title":"Creating Instance","text":"<p><code>CompleteFlow</code> takes the following to be created:</p> <ul> <li> UnresolvedFlow <li> FlowFunctionResult <li> <code>mustBeAppend</code> flag (default: <code>false</code>) <p><code>CompleteFlow</code> is created when:</p> <ul> <li><code>FlowResolver</code> is requested to convertResolvedToTypedFlow (for UnresolvedFlows that are neither once nor their results are streaming dataframes)</li> </ul>"},{"location":"CoreDataflowNodeProcessor/","title":"CoreDataflowNodeProcessor","text":""},{"location":"CoreDataflowNodeProcessor/#creating-instance","title":"Creating Instance","text":"<p><code>CoreDataflowNodeProcessor</code> takes the following to be created:</p> <ul> <li> DataflowGraph <p><code>CoreDataflowNodeProcessor</code> is created when:</p> <ul> <li><code>DataflowGraph</code> is requested to resolve</li> </ul>"},{"location":"CoreDataflowNodeProcessor/#flowResolver","title":"FlowResolver","text":"<p><code>CoreDataflowNodeProcessor</code> creates a FlowResolver when created.</p> <p>The <code>FlowResolver</code> is used to process an UnresolvedFlow.</p>"},{"location":"CoreDataflowNodeProcessor/#processNode","title":"processNode","text":"<pre><code>processNode(\n  node: GraphElement,\n  upstreamNodes: Seq[GraphElement]): Seq[GraphElement]\n</code></pre> <p><code>processNode</code>...FIXME</p> <p><code>processNode</code> is used when:</p> <ul> <li><code>DataflowGraph</code> is requested to resolve</li> </ul>"},{"location":"CoreDataflowNodeProcessor/#processUnresolvedFlow","title":"processUnresolvedFlow","text":"<pre><code>processUnresolvedFlow(\n  flow: UnresolvedFlow): ResolvedFlow\n</code></pre> <p><code>processUnresolvedFlow</code>...FIXME</p>"},{"location":"DataflowGraph/","title":"DataflowGraph","text":"<p><code>DataflowGraph</code> is a GraphRegistrationContext with tables, sinks, views and flows fully-qualified, resolved and de-duplicated.</p>"},{"location":"DataflowGraph/#creating-instance","title":"Creating Instance","text":"<p><code>DataflowGraph</code> takes the following to be created:</p> <ul> <li> Flows <li> Tables <li> Sinks <li> Views <p><code>DataflowGraph</code> is created when:</p> <ul> <li><code>DataflowGraph</code> is requested to reanalyzeFlow</li> <li><code>GraphRegistrationContext</code> is requested to convert to a DataflowGraph</li> </ul>"},{"location":"DataflowGraph/#output","title":"Outputs","text":"<pre><code>output: Map[TableIdentifier, Output]\n</code></pre> Lazy Value <p><code>output</code> is a Scala lazy value to guarantee that the code to initialize it is executed once only (when accessed for the first time) and the computed value never changes afterwards.</p> <p>Learn more in the Scala Language Specification.</p> <p><code>output</code> is a collection of unique <code>Output</code>s (tables and sinks) by their <code>TableIdentifier</code>.</p> <p><code>output</code> is used when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a flow for execution (to find the destination table of a flow)</li> <li><code>DataflowGraph</code> is requested for the materialized flows</li> </ul>"},{"location":"DataflowGraph/#reanalyzeFlow","title":"reanalyzeFlow","text":"<pre><code>reanalyzeFlow(\n  srcFlow: Flow): ResolvedFlow\n</code></pre> <p><code>reanalyzeFlow</code> finds the upstream datasets.</p> <p><code>reanalyzeFlow</code> finds the upstream flows (for the upstream datasets that could be found in the resolvedFlows registry).</p> <p><code>reanalyzeFlow</code> finds the upstream views (for the upstream datasets that could be found in the view registry).</p> <p><code>reanalyzeFlow</code> creates a new (sub)DataflowGraph for the upstream flows, views and a single table (the destination of the given Flow).</p> <p><code>reanalyzeFlow</code> requests the subgraph to resolve and returns the ResolvedFlow for the given Flow.</p> <p><code>reanalyzeFlow</code> is used when:</p> <ul> <li><code>BatchTableWrite</code> is requested to executeAsync (and executeInternal)</li> <li><code>StreamingTableWrite</code> is requested to executeAsync (and startStream)</li> </ul>"},{"location":"DataflowGraph/#resolve","title":"Resolve","text":"<pre><code>resolve(): DataflowGraph\n</code></pre> <p><code>resolve</code>...FIXME</p> <p><code>resolve</code> is used when:</p> <ul> <li><code>DataflowGraph</code> is requested to reanalyzeFlow</li> <li><code>PipelineExecution</code> is requested to initializeGraph</li> </ul>"},{"location":"DataflowGraph/#validate","title":"Validate","text":"<pre><code>validate(): DataflowGraph\n</code></pre> <p><code>validate</code> raises the exception (available through the validationFailure), if reported, or returns this <code>DataflowGraph</code>.</p> <p><code>validate</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to resolve and validate a dataflow graph</li> </ul>"},{"location":"DataflowGraphRegistry/","title":"DataflowGraphRegistry","text":"<p><code>DataflowGraphRegistry</code> is a registry of Dataflow Graphs (a mere wrapper around a collection of GraphRegistrationContexts)</p> <p>Scala object</p> <p><code>DataflowGraphRegistry</code> is an <code>object</code> in Scala which means it is a class that has exactly one instance (itself). A Scala <code>object</code> is created lazily when it is referenced for the first time.</p> <p>Learn more in Tour of Scala.</p>"},{"location":"DataflowGraphRegistry/#demo","title":"Demo","text":"<pre><code>import org.apache.spark.sql.connect.pipelines.DataflowGraphRegistry\n\nval graphId = DataflowGraphRegistry.createDataflowGraph(\n  defaultCatalog=spark.catalog.currentCatalog(),\n  defaultDatabase=spark.catalog.currentDatabase,\n  defaultSqlConf=Map.empty)\n</code></pre> <pre><code>assert(DataflowGraphRegistry.getAllDataflowGraphs.size == 1)\n</code></pre>"},{"location":"DataflowGraphRegistry/#dataflowGraphs","title":"Dataflow Graphs","text":"<pre><code>dataflowGraphs: ConcurrentHashMap[String, GraphRegistrationContext]\n</code></pre> <p><code>DataflowGraphRegistry</code> manages GraphRegistrationContexts (by graph IDs).</p> <p>A new GraphRegistrationContext is added when <code>DataflowGraphRegistry</code> is requested to create a new dataflow graph.</p> <p>A single GraphRegistrationContext can be looked up with getDataflowGraph and getDataflowGraphOrThrow.</p> <p>All the GraphRegistrationContexts can be returned with getAllDataflowGraphs.</p> <p>A GraphRegistrationContext is removed when dropDataflowGraph.</p> <p><code>dataflowGraphs</code> is cleared up with dropAllDataflowGraphs.</p>"},{"location":"DataflowGraphRegistry/#createDataflowGraph","title":"Create Dataflow Graph","text":"<pre><code>createDataflowGraph(\n  defaultCatalog: String,\n  defaultDatabase: String,\n  defaultSqlConf: Map[String, String]): String\n</code></pre> <p><code>createDataflowGraph</code> generates a graph ID (as a pseudo-randomly generated UUID).</p> <p><code>createDataflowGraph</code> registers a new GraphRegistrationContext with the graph ID (in this dataflowGraphs registry).</p> <p>In the end, <code>createDataflowGraph</code> returns the graph ID.</p> <p><code>createDataflowGraph</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to create a dataflow graph</li> </ul>"},{"location":"DataflowGraphRegistry/#getDataflowGraphOrThrow","title":"Find Dataflow Graph (or Throw SparkException)","text":"<pre><code>getDataflowGraphOrThrow(\n  dataflowGraphId: String): GraphRegistrationContext\n</code></pre> <p><code>getDataflowGraphOrThrow</code> looks up the GraphRegistrationContext for the given <code>dataflowGraphId</code> or throws an <code>SparkException</code> if it does not exist.</p> <pre><code>Dataflow graph with id [graphId] could not be found\n</code></pre> <p><code>getDataflowGraphOrThrow</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> (Spark Connect) is requested to defineDataset, defineFlow, defineSqlGraphElements, startRun</li> </ul>"},{"location":"DataflowGraphRegistry/#getDataflowGraph","title":"Find Dataflow Graph","text":"<pre><code>getDataflowGraph(\n  graphId: String): Option[GraphRegistrationContext]\n</code></pre> <p><code>getDataflowGraph</code> finds the GraphRegistrationContext for the given <code>graphId</code> (in this dataflowGraphs registry).</p> <p><code>getDataflowGraph</code> is used when:</p> <ul> <li><code>DataflowGraphRegistry</code> is requested to getDataflowGraphOrThrow</li> </ul>"},{"location":"DatasetManager/","title":"DatasetManager","text":"<p><code>DatasetManager</code> is a global manager to materialize datasets (tables and persistent views) right after a pipeline update.</p> <p></p> Materialization <p>Materialization is a process of publishing tables and persistent views to session <code>TableCatalog</code> (Spark SQL) and <code>SessionCatalog</code> (Spark SQL), for tables and persistent views, respectively.</p> Scala object <p><code>DatasetManager</code> is an <code>object</code> in Scala which means it is a class that has exactly one instance (itself). A Scala <code>object</code> is created lazily when it is referenced for the first time.</p> <p>Learn more in Tour of Scala.</p>"},{"location":"DatasetManager/#materializeDatasets","title":"Materialize Datasets","text":"<pre><code>materializeDatasets(\n  resolvedDataflowGraph: DataflowGraph,\n  context: PipelineUpdateContext): DataflowGraph\n</code></pre> <p><code>materializeDatasets</code> constructFullRefreshSet for the tables in the given DataflowGraph (and the PipelineUpdateContext).</p> <p><code>materializeDatasets</code> marks the tables (in the given DataflowGraph) as to be refreshed and fully-refreshed.</p> <p><code>materializeDatasets</code>...FIXME</p> <p>For every table to be materialized, <code>materializeDatasets</code> materializeTable.</p> <p>In the end, <code>materializeDatasets</code> materializeViews.</p> <p><code>materializeDatasets</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to initialize the dataflow graph</li> </ul>"},{"location":"DatasetManager/#materializeTable","title":"Materialize Table","text":"<pre><code>materializeTable(\n  resolvedDataflowGraph: DataflowGraph,\n  table: Table,\n  isFullRefresh: Boolean,\n  context: PipelineUpdateContext): Table\n</code></pre> <p><code>materializeTable</code> prints out the following INFO message to the logs:</p> <pre><code>Materializing metadata for table [identifier].\n</code></pre> <p><code>materializeTable</code> uses the given PipelineUpdateContext to find the <code>CatalogManager</code> (Spark SQL) (in the SparkSession).</p> <p><code>materializeTable</code> finds the <code>TableCatalog</code> (Spark SQL) for the table.</p> <p><code>materializeTable</code> requests the <code>TableCatalog</code> (Spark SQL) to load the table if exists (Spark SQL) already.</p> <p>For an existing table, <code>materializeTable</code> wipes data out (<code>TRUNCATE TABLE</code>) if it is <code>isFullRefresh</code> or the table is not streaming.</p> <p>For an existing table, <code>materializeTable</code> requests the <code>TableCatalog</code> (Spark SQL) to alter the table if there are any changes in the schema or table properties.</p> <p>Unless created already, <code>materializeTable</code> requests the <code>TableCatalog</code> (Spark SQL) to create the table.</p> <p>In the end, <code>materializeTable</code> requests the <code>TableCatalog</code> (Spark SQL) to load the materialized table and returns the given Table back (with the normalized table storage path updated to the <code>location</code> property of the materialized table).</p>"},{"location":"DatasetManager/#materializeViews","title":"Materialize Views","text":"<pre><code>materializeViews(\n  virtualizedConnectedGraphWithTables: DataflowGraph,\n  context: PipelineUpdateContext): Unit\n</code></pre> <p><code>materializeViews</code> requests the given DataflowGraph for the persisted views to materialize (publish or refresh).</p> <p>Publish (Materialize) Views</p> <p>To publish a view, it is required that all the input sources must exist in the metastore. If a Persisted View target reads another Persisted View source, the source must be published first.</p> <p><code>materializeViews</code>...FIXME</p> <p>For each view to be persisted (with no pending inputs), <code>materializeViews</code> materialize the view.</p>"},{"location":"DatasetManager/#materializeView","title":"Materialize View","text":"<pre><code>materializeView(\n  view: View,\n  flow: ResolvedFlow,\n  spark: SparkSession): Unit\n</code></pre> <p><code>materializeView</code> executes a <code>CreateViewCommand</code> (Spark SQL) logical command.</p> <p><code>materializeView</code> creates a <code>CreateViewCommand</code> (Spark SQL) logical command (as a <code>PersistedView</code> with <code>allowExisting</code> and <code>replace</code> flags enabled).</p> <p><code>materializeView</code> requests the given ResolvedFlow for the QueryContext to set the current catalog and current database, if defined, in the session <code>CatalogManager</code> (Spark SQL).</p> <p>In the end, <code>materializeView</code> executes the <code>CreateViewCommand</code> (Spark SQL).</p>"},{"location":"DatasetManager/#constructFullRefreshSet","title":"constructFullRefreshSet","text":"<pre><code>constructFullRefreshSet(\n  graphTables: Seq[Table],\n  context: PipelineUpdateContext): (Seq[Table], Seq[TableIdentifier], Seq[TableIdentifier])\n</code></pre> <p><code>constructFullRefreshSet</code> gives the following collections:</p> <ul> <li>Tables to be refreshed (incl. a full refresh)</li> <li><code>TableIdentifier</code>s of the tables to be refreshed (excl. fully refreshed)</li> <li><code>TableIdentifier</code>s of the tables to be fully refreshed only</li> </ul> <p>If there are tables to be fully refreshed yet not allowed for a full refresh, <code>constructFullRefreshSet</code> prints out the following INFO message to the logs:</p> <pre><code>Skipping full refresh on some tables because pipelines.reset.allowed was set to false.\nTables: [fullRefreshNotAllowed]\n</code></pre> <p><code>constructFullRefreshSet</code>...FIXME</p> <p><code>constructFullRefreshSet</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to initialize the dataflow graph</li> </ul>"},{"location":"Flow/","title":"Flow","text":"<p><code>Flow</code> is an extension of the GraphElement abstraction for flows in dataflow graphs.</p> <p>Flows can be batch or streaming.</p> <p>Flows can be defined explicitly or implicitly (while defining other pipeline elements) in SQL and Python.</p> <p>Flows must be successfully analyzed (resolved) in order to determine whether they are streaming or not.</p> <p>Flows and DataFrames</p> <p>Think of flows as Spark DataFrames (that declaratively describe computations over batch or streaming data sources in Apache Spark).</p>"},{"location":"Flow/#contract-subset","title":"Contract (Subset)","text":""},{"location":"Flow/#func","title":"FlowFunction","text":"<pre><code>func: FlowFunction\n</code></pre> <p>FlowFunction of this <code>Flow</code></p> <p>Used to create an UnresolvedFlow</p> <p>See:</p> <ul> <li>ResolutionCompletedFlow</li> <li>UnresolvedFlow</li> </ul>"},{"location":"Flow/#once","title":"once","text":"<pre><code>once: Boolean\n</code></pre> <p>One-time flows Unsupported</p> <p>One-time flows are not supported yet (and defineFlow reports an <code>AnalysisException</code> for <code>DefineFlow</code>s with <code>once</code> enabled).</p> <p>Indicates whether this is a ONCE flow or not. ONCE flows can only be run once per full refresh.</p> <ul> <li>ONCE flows are planned for execution as AppendOnceFlows (FlowResolver)</li> <li>ONCE flows are marked as IDLE when <code>TriggeredGraphExecution</code> is requested to start flows in topologicalExecution.</li> <li>ONCE flows must be batch (not streaming).</li> <li>For ONCE flows or when the logical plan for the flow is streaming, <code>GraphElementTypeUtils</code> considers a ResolvedFlow as a <code>STREAMING_TABLE</code> (in getDatasetTypeForMaterializedViewOrStreamingTable).</li> </ul> <p>Default: <code>false</code></p> <p>See:</p> <ul> <li>AppendOnceFlow</li> <li>UnresolvedFlow</li> </ul> <p>Used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to topologicalExecution</li> <li><code>PipelinesErrors</code> is requested to checkStreamingErrorsAndRetry (to skip ONCE flows with no exception)</li> <li><code>GraphValidations</code> is requested to validateFlowStreamingness</li> <li><code>GraphElementTypeUtils</code> is requested to getDatasetTypeForMaterializedViewOrStreamingTable</li> <li><code>FlowResolver</code> is requested to convertResolvedToTypedFlow</li> </ul>"},{"location":"Flow/#implementations","title":"Implementations","text":"<ul> <li>ResolutionCompletedFlow</li> <li>UnresolvedFlow</li> </ul>"},{"location":"FlowAnalysis/","title":"FlowAnalysis Utility","text":"Singleton Object <p><code>FlowAnalysis</code> is a Scala object which is a class that has exactly one instance. It is created lazily when it is referenced, like a <code>lazy val</code>.</p> <p>Learn more in Tour of Scala.</p>"},{"location":"FlowAnalysis/#createFlowFunctionFromLogicalPlan","title":"createFlowFunctionFromLogicalPlan","text":"<pre><code>createFlowFunctionFromLogicalPlan(\n  plan: LogicalPlan): FlowFunction\n</code></pre> <p><code>createFlowFunctionFromLogicalPlan</code> takes a LogicalPlan (that represents one of the supported logical commands) and creates a FlowFunction.</p> <p>When executed, this <code>FlowFunction</code> creates a FlowAnalysisContext.</p> <p><code>FlowFunction</code> uses this <code>FlowAnalysisContext</code> to set the SQL configs (given to the FlowFunction being defined).</p> <p><code>FlowFunction</code> analyze this <code>LogicalPlan</code> (with the <code>FlowAnalysisContext</code>). This gives the result data (as a <code>DataFrame</code>).</p> <p>In the end, <code>FlowFunction</code> creates a FlowFunctionResult with the result data (as a DataFrame) and the others (from the FlowAnalysisContext).</p> <p><code>createFlowFunctionFromLogicalPlan</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a flow</li> <li><code>SqlGraphRegistrationContext</code> is requested to handle the following queries:<ul> <li>CreateFlowCommand</li> <li>CreateMaterializedViewAsSelect</li> <li>CreateView</li> <li>CreateStreamingTableAsSelect</li> <li>CreateViewCommand</li> </ul> </li> </ul>"},{"location":"FlowAnalysis/#analyze","title":"Analyze Logical Command","text":"<pre><code>analyze(\n  context: FlowAnalysisContext,\n  plan: LogicalPlan): DataFrame\n</code></pre> <p>CTEs</p> <p><code>analyze</code> resolves pipeline-specific TVFs and CTEs.</p> <pre><code>SELECT ... FROM STREAM(t1)\nSELECT ... FROM STREAM t1\n</code></pre> <p>Developers can define CTEs within their CREATE statements:</p> <pre><code>CREATE STREAMING TABLE a\nWITH b AS (\n   SELECT * FROM STREAM upstream\n)\nSELECT * FROM b\n</code></pre> <p><code>analyze</code>...FIXME</p>"},{"location":"FlowAnalysis/#readBatchInput","title":"Read Batch Input","text":"<pre><code>readBatchInput(\n  context: FlowAnalysisContext,\n  name: String,\n  batchReadOptions: BatchReadOptions): DataFrame\n</code></pre> <p><code>readBatchInput</code>...FIXME</p> <p><code>readBatchInput</code> is used when:</p> <ul> <li><code>FlowAnalysis</code> is requested to analyze</li> </ul>"},{"location":"FlowAnalysis/#readExternalBatchInput","title":"Read External Batch Input","text":"<pre><code>readExternalBatchInput(\n  context: FlowAnalysisContext,\n  inputIdentifier: ExternalDatasetIdentifier,\n  name: String): DataFrame\n</code></pre> <p><code>readExternalBatchInput</code>...FIXME</p>"},{"location":"FlowAnalysis/#readStreamInput","title":"Read Stream Input","text":"<pre><code>readStreamInput(\n  context: FlowAnalysisContext,\n  name: String,\n  streamReader: DataStreamReader,\n  streamingReadOptions: StreamingReadOptions): DataFrame\n</code></pre> <p><code>readStreamInput</code> resolves the given <code>name</code> (in the given FlowAnalysisContext).</p> <p>For an <code>InternalDatasetIdentifier</code> (that is defined by the current pipeline), <code>readStreamInput</code> readGraphInput.</p> <p>For an <code>ExternalDatasetIdentifier</code> (that is external to the current pipeline), <code>readStreamInput</code> readExternalStreamInput.</p> <p><code>readStreamInput</code> is used when:</p> <ul> <li><code>FlowAnalysis</code> is requested to analyze</li> </ul>"},{"location":"FlowAnalysis/#readExternalStreamInput","title":"Read External Stream Input","text":"<pre><code>readExternalStreamInput(\n  context: FlowAnalysisContext,\n  inputIdentifier: ExternalDatasetIdentifier,\n  streamReader: DataStreamReader,\n  name: String): DataFrame\n</code></pre> <p><code>readExternalStreamInput</code>...FIXME</p>"},{"location":"FlowAnalysis/#readGraphInput","title":"Read Graph Input","text":"<pre><code>readGraphInput(\n  ctx: FlowAnalysisContext,\n  inputIdentifier: InternalDatasetIdentifier,\n  readOptions: InputReadOptions): DataFrame\n</code></pre> <p>Load DataFrame</p> <p>It is up to the Input (for the given <code>InternalDatasetIdentifier</code>) to load a DataFrame that may either be batch or streaming.</p> <p><code>readGraphInput</code> records the input dataset identifier in the given FlowAnalysisContext.</p> SparkException <p>For a dataset not defined in the dataflow graph (the given <code>InternalDatasetIdentifier</code> not being available in the FlowAnalysisContext), <code>readGraphInput</code> reports a <code>SparkException</code>.</p> <p><code>readGraphInput</code> finds the Input for the given <code>InternalDatasetIdentifier</code> (in the FlowAnalysisContext).</p> <p><code>readGraphInput</code> requests the <code>Input</code> to load a DataFrame (with the given InputReadOptions).</p> <p><code>readGraphInput</code> records a <code>ResolvedInput</code> in the FlowAnalysisContext (in streamingInputs or batchInputs for <code>StreamingReadOptions</code> or <code>BatchReadOptions</code>, respectively).</p> <p>In the end, <code>readGraphInput</code> creates a (streaming or batch) <code>Dataset</code>.</p> <p><code>readGraphInput</code> is used when:</p> <ul> <li><code>FlowAnalysis</code> is requested to read batch and stream input</li> </ul>"},{"location":"FlowAnalysisContext/","title":"FlowAnalysisContext","text":"<p><code>FlowAnalysisContext</code> is...FIXME</p>"},{"location":"FlowExecution/","title":"FlowExecution","text":"<p><code>FlowExecution</code> is an abstraction of flow executions (batch or streaming).</p> <p><code>FlowExecution</code> can be executed asynchronously.</p>"},{"location":"FlowExecution/#contract","title":"Contract","text":""},{"location":"FlowExecution/#identifier","title":"TableIdentifier","text":"<pre><code>identifier: TableIdentifier\n</code></pre>"},{"location":"FlowExecution/#destination","title":"Destination","text":"<pre><code>destination: Output\n</code></pre> <p>Output</p> <p>See:</p> <ul> <li>BatchTableWrite</li> <li>SinkWrite</li> <li>StreamingTableWrite</li> </ul>"},{"location":"FlowExecution/#getOrigin","title":"QueryOrigin","text":"<pre><code>getOrigin: QueryOrigin\n</code></pre>"},{"location":"FlowExecution/#isStreaming","title":"isStreaming","text":"<pre><code>isStreaming: Boolean\n</code></pre> <p>See:</p> <ul> <li>BatchTableWrite</li> <li>StreamingFlowExecution</li> </ul>"},{"location":"FlowExecution/#updateContext","title":"PipelineUpdateContext","text":"<pre><code>updateContext: PipelineUpdateContext\n</code></pre> <p>PipelineUpdateContext of this flow execution</p>"},{"location":"FlowExecution/#executeInternal","title":"executeInternal","text":"<pre><code>executeInternal(): Future[Unit]\n</code></pre> <p>See:</p> <ul> <li>BatchTableWrite</li> <li>StreamingFlowExecution</li> </ul> <p>Used when:</p> <ul> <li><code>FlowExecution</code> is requested to execute asynchronously</li> </ul>"},{"location":"FlowExecution/#implementations","title":"Implementations","text":"<ul> <li>BatchTableWrite</li> <li>StreamingFlowExecution</li> </ul>"},{"location":"FlowExecution/#executeAsync","title":"Execute Asynchronously","text":"<pre><code>executeAsync(): Unit\n</code></pre> <p><code>executeAsync</code> uses the internal _future to control whether this flow has been executed or not.</p> <p><code>executeAsync</code> executeInternal (and stores the result of this execution in the internal _future).</p> Final Method <p><code>executeAsync</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p> <p><code>executeAsync</code> is used when:</p> <ul> <li><code>GraphExecution</code> is requested to planAndStartFlow</li> </ul>"},{"location":"FlowFunction/","title":"FlowFunction","text":"<p><code>FlowFunction</code> is an abstraction of transformation functions that define flows.</p> <p><code>FlowFunction</code> is created when <code>FlowAnalysis</code> is requested to createFlowFunctionFromLogicalPlan.</p>"},{"location":"FlowFunction/#contract","title":"Contract","text":""},{"location":"FlowFunction/#call","title":"call","text":"<pre><code>call(\n  allInputs: Set[TableIdentifier],\n  availableInputs: Seq[Input],\n  configuration: Map[String, String],\n  queryContext: QueryContext,\n  queryOrigin: QueryOrigin): FlowFunctionResult\n</code></pre> <p>Transformation that yields a FlowFunctionResult (with the data, if successful)</p> <p>Used when:</p> <ul> <li><code>FlowResolver</code> is requested to attemptResolveFlow</li> </ul>"},{"location":"FlowFunctionResult/","title":"FlowFunctionResult","text":"<p><code>FlowFunctionResult</code> is the result of executing a FlowFunction.</p> <p><code>FlowFunctionResult</code> is a part of ResolutionCompletedFlows.</p>"},{"location":"FlowFunctionResult/#creating-instance","title":"Creating Instance","text":"<p><code>FlowFunctionResult</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code>s of the requested inputs <li> <code>ResolvedInput</code>s of the batch inputs <li> <code>ResolvedInput</code>s of the streaming inputs <li> <code>TableIdentifier</code> of the external inputs <li>DataFrame</li> <li> SQL Configuration <li> <code>AnalysisWarning</code> (default: undefined) <p><code>FlowFunctionResult</code> is created when:</p> <ul> <li><code>FlowAnalysis</code> is requested to createFlowFunctionFromLogicalPlan</li> </ul>"},{"location":"FlowFunctionResult/#dataFrame","title":"DataFrame","text":"<p><code>FlowFunctionResult</code> is given a DataFrame (produced by the corresponding flow) when created.</p> <p>When this <code>DataFrame</code> is streaming, <code>FlowResolver</code> converts an UnresolvedFlow to a StreamingFlow.</p>"},{"location":"FlowFunctionResult/#inputs","title":"Inputs","text":"<pre><code>inputs: Set[TableIdentifier]\n</code></pre> <p><code>inputs</code> are all the <code>TableIdentifier</code>s of the Inputs of this batchInputs and streamingInputs.</p> <p><code>inputs</code> is used when:</p> <ul> <li><code>ResolvedFlow</code> is requested for the inputs</li> </ul>"},{"location":"FlowPlanner/","title":"FlowPlanner","text":"<p><code>FlowPlanner</code> is used to plan a flow into a FlowExecution.</p> <p><code>FlowPlanner</code> is created alongside a GraphExecution (i.e., when PipelineExecution is requested to start a pipeline).</p>"},{"location":"FlowPlanner/#creating-instance","title":"Creating Instance","text":"<p><code>FlowPlanner</code> takes the following to be created:</p> <ul> <li> DataflowGraph <li> PipelineUpdateContext <li> Flow-to-Streaming-Trigger Conversion Function <p><code>FlowPlanner</code> is created alongside a GraphExecution.</p>"},{"location":"FlowPlanner/#triggerFor","title":"Flow-to-Streaming-Trigger Conversion Function","text":"<pre><code>triggerFor: Flow =&gt; Trigger\n</code></pre> <p><code>FlowPlanner</code> is given a function to convert a Flow into a streaming <code>Trigger</code> (Spark Structured Streaming) when created.</p> <p>The <code>triggerFor</code> function is the streamTrigger function of the owning GraphExecution.</p>"},{"location":"FlowPlanner/#plan","title":"Plan Flow for Execution","text":"<pre><code>plan(\n  flow: ResolvedFlow): FlowExecution\n</code></pre> <p><code>plan</code> looks up the output for the destination identifier of the given ResolvedFlow in this DataflowGraph.</p> <p><code>plan</code> creates a FlowExecution (for the given ResolvedFlow and the Output) as follows:</p> FlowExecution ResolvedFlow Output BatchTableWrite CompleteFlow Table SinkWrite StreamingFlow Sink StreamingTableWrite StreamingFlow Table <p><code>plan</code> is used when:</p> <ul> <li><code>GraphExecution</code> is requested to planAndStartFlow</li> </ul>"},{"location":"FlowProgressEventLogger/","title":"FlowProgressEventLogger","text":"<p><code>FlowProgressEventLogger</code> is...FIXME</p>"},{"location":"FlowResolver/","title":"FlowResolver","text":""},{"location":"FlowResolver/#creating-instance","title":"Creating Instance","text":"<p><code>FlowResolver</code> takes the following to be created:</p> <ul> <li> DataflowGraph <p><code>FlowResolver</code> is created alongside a CoreDataflowNodeProcessor.</p>"},{"location":"FlowResolver/#attemptResolveFlow","title":"attemptResolveFlow","text":"<pre><code>attemptResolveFlow(\n  flowToResolve: UnresolvedFlow,\n  allInputs: Set[TableIdentifier],\n  availableResolvedInputs: Map[TableIdentifier, Input]): ResolvedFlow\n</code></pre> <p><code>attemptResolveFlow</code>...FIXME</p> <p><code>attemptResolveFlow</code> is used when:</p> <ul> <li><code>CoreDataflowNodeProcessor</code> is requested to processUnresolvedFlow</li> </ul>"},{"location":"FlowResolver/#convertResolvedToTypedFlow","title":"convertResolvedToTypedFlow","text":"<pre><code>convertResolvedToTypedFlow(\n  flow: UnresolvedFlow,\n  funcResult: FlowFunctionResult): ResolvedFlow\n</code></pre> <p><code>convertResolvedToTypedFlow</code> converts the given UnresolvedFlow as follows (and in that order):</p> <ul> <li>AppendOnceFlow for a once flow</li> <li>StreamingFlow for the given FlowFunctionResult with a streaming <code>DataFrame</code></li> <li>CompleteFlow, otherwise</li> </ul>"},{"location":"FlowSystemMetadata/","title":"FlowSystemMetadata","text":"<p><code>FlowSystemMetadata</code> is a SystemMetadata associated with a Flow.</p>"},{"location":"FlowSystemMetadata/#creating-instance","title":"Creating Instance","text":"<p><code>FlowSystemMetadata</code> takes the following to be created:</p> <ul> <li> PipelineUpdateContext <li> Flow <li> DataflowGraph <p><code>FlowSystemMetadata</code> is created when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a StreamingFlow for execution</li> <li><code>State</code> is requested to clear out the state of a flow</li> </ul>"},{"location":"FlowSystemMetadata/#latestCheckpointLocation","title":"latestCheckpointLocation","text":"<pre><code>latestCheckpointLocation: String\n</code></pre> <p><code>latestCheckpointLocation</code>...FIXME</p> <p><code>latestCheckpointLocation</code> is used when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a StreamingFlow</li> <li><code>State</code> is requested to reset a flow</li> </ul>"},{"location":"GraphElement/","title":"GraphElement","text":"<p><code>GraphElement</code> is an abstraction of dataflow graph elements that have an identifier and an origin.</p>"},{"location":"GraphElement/#contract","title":"Contract","text":""},{"location":"GraphElement/#identifier","title":"Identifier","text":"<pre><code>identifier: TableIdentifier\n</code></pre>"},{"location":"GraphElement/#origin","title":"QueryOrigin","text":"<pre><code>origin: QueryOrigin\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a flow and an output</li> <li><code>SqlGraphRegistrationContext</code> is requested to process a single SQL query</li> <li><code>FlowResolver</code> is requested to attemptResolveFlow</li> <li><code>DatasetManager</code> is requested to materialize datasets (for error reporting)</li> <li><code>FlowExecution</code> is requested for the QueryOrigin</li> <li>FlowProgressEventLogger is requested to report an event</li> </ul>"},{"location":"GraphElement/#implementations","title":"Implementations","text":"<ul> <li>Flow</li> <li>Input</li> <li>Sink</li> <li>View</li> </ul>"},{"location":"GraphElementRegistry/","title":"GraphElementRegistry","text":"<p><code>GraphElementRegistry</code> is an abstraction of graph element registries:</p> <ul> <li>Outputs</li> <li>Flows</li> </ul> <p>Graph elements can be defined in Python and SQL.</p>"},{"location":"GraphElementRegistry/#contract","title":"Contract","text":""},{"location":"GraphElementRegistry/#register_output","title":"Register Output","text":"<pre><code>register_output(\n    self,\n    output: Output,\n) -&gt; None\n</code></pre> <p>Registers the given Output</p> <p>See:</p> <ul> <li>SparkConnectGraphElementRegistry</li> </ul> <p>Used for the following:</p> <ul> <li>dp.create_sink</li> <li>@dp.create_streaming_table</li> <li>@dp.table</li> <li>@dp.materialized_view</li> <li>@dp.temporary_view</li> </ul>"},{"location":"GraphElementRegistry/#register_flow","title":"Register Flow","text":"<pre><code>register_flow(\n    self,\n    flow: Flow,\n) -&gt; None\n</code></pre> <p>Registers the given Flow</p> <p>See:</p> <ul> <li>SparkConnectGraphElementRegistry</li> </ul> <p>Used for the following:</p> <ul> <li>@dp.append_flow</li> <li>@dp.table</li> <li>@dp.materialized_view</li> <li>@dp.temporary_view</li> </ul>"},{"location":"GraphElementRegistry/#register_sql","title":"Register SQL File","text":"<pre><code>register_sql(\n    self,\n    sql_text: str,\n    file_path: Path,\n) -&gt; None\n</code></pre> <p>See:</p> <ul> <li>SparkConnectGraphElementRegistry</li> </ul> <p>Used when:</p> <ul> <li>Pipelines CLI is requested to register graph element definitions (from SQL files)</li> </ul>"},{"location":"GraphElementRegistry/#implementations","title":"Implementations","text":"<ul> <li>SparkConnectGraphElementRegistry</li> </ul>"},{"location":"GraphElementTypeUtils/","title":"GraphElementTypeUtils","text":""},{"location":"GraphElementTypeUtils/#getDatasetTypeForMaterializedViewOrStreamingTable","title":"getDatasetTypeForMaterializedViewOrStreamingTable","text":"<pre><code>getDatasetTypeForMaterializedViewOrStreamingTable(\n  flowsToTable: Seq[ResolvedFlow]): DatasetType\n</code></pre> <p><code>getDatasetTypeForMaterializedViewOrStreamingTable</code>...FIXME</p> <p><code>getDatasetTypeForMaterializedViewOrStreamingTable</code> is used when:</p> <ul> <li><code>GraphValidations</code> is requested to validateUserSpecifiedSchemas</li> <li><code>SchemaInferenceUtils</code> is requested to inferSchemaFromFlows</li> </ul>"},{"location":"GraphExecution/","title":"GraphExecution","text":"<p><code>GraphExecution</code> is an abstraction of graph executors that can...FIXME</p>"},{"location":"GraphExecution/#contract","title":"Contract (Subset)","text":""},{"location":"GraphExecution/#awaitCompletion","title":"awaitCompletion","text":"<pre><code>awaitCompletion(): Unit\n</code></pre> <p>See:</p> <ul> <li>TriggeredGraphExecution</li> </ul> <p>Used when:</p> <ul> <li><code>PipelineExecution</code> is requested to await completion</li> </ul>"},{"location":"GraphExecution/#streamTrigger","title":"streamTrigger","text":"<pre><code>streamTrigger(\n  flow: Flow): Trigger\n</code></pre> <p>See:</p> <ul> <li>TriggeredGraphExecution</li> </ul> <p>Used when:</p> <ul> <li><code>GraphExecution</code> is created (to create the FlowPlanner)</li> </ul>"},{"location":"GraphExecution/#implementations","title":"Implementations","text":"<ul> <li>TriggeredGraphExecution</li> </ul>"},{"location":"GraphExecution/#creating-instance","title":"Creating Instance","text":"<p><code>GraphExecution</code> takes the following to be created:</p> <ul> <li> DataflowGraph <li> PipelineUpdateContext Abstract Class <p><code>GraphExecution</code> is an abstract class and cannot be created directly. It is created indirectly for the concrete graph executors.</p>"},{"location":"GraphExecution/#flowPlanner","title":"FlowPlanner","text":"<p><code>GraphExecution</code> creates a FlowPlanner when created.</p> <p>This <code>FlowPlanner</code> is created for this DataflowGraph and this PipelineUpdateContext, with a Trigger (that is supposed to be defined by the implementations).</p> <p>This <code>FlowPlanner</code> is used when <code>GraphExecution</code> is requested to plan and start a flow.</p>"},{"location":"GraphExecution/#start","title":"Start","text":"<pre><code>start(): Unit\n</code></pre> <p><code>start</code> requests the session-bound ExecutionListenerManager to remove all QueryExecutionListeners.</p> <p>In the end, <code>start</code> registers this StreamListener with the session-bound StreamingQueryManager.</p> <p><code>start</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to start the pipeline</li> </ul>"},{"location":"GraphExecution/#planAndStartFlow","title":"planAndStartFlow","text":"<pre><code>planAndStartFlow(\n    flow: ResolvedFlow): Option[FlowExecution]\n</code></pre> <p><code>planAndStartFlow</code>...FIXME</p> <p><code>planAndStartFlow</code> is used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to topologicalExecution</li> </ul>"},{"location":"GraphExecution/#streamListener","title":"StreamListener","text":"<p><code>GraphExecution</code> creates a new StreamListener when created.</p> <p>The <code>StreamListener</code> is created for this PipelineUpdateContext and DataflowGraph.</p> <p>The <code>StreamListener</code> is registered (added) to the session-bound StreamingQueryManager when started, and deregistered (removed) when stopped.</p>"},{"location":"GraphExecution/#stop","title":"Stop","text":"<pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> requests this session-bound StreamingQueryManager to remove this StreamListener.</p> <p><code>stop</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to stop the pipeline</li> <li><code>TriggeredGraphExecution</code> is requested to create the Topological Execution thread and stopInternal</li> </ul>"},{"location":"GraphIdentifierManager/","title":"GraphIdentifierManager Utility","text":""},{"location":"GraphIdentifierManager/#parseAndQualifyInputIdentifier","title":"parseAndQualifyInputIdentifier","text":"<pre><code>parseAndQualifyInputIdentifier(\n  context: FlowAnalysisContext,\n  rawInputName: String): DatasetIdentifier\n</code></pre> <p><code>parseAndQualifyInputIdentifier</code>...FIXME</p> <p><code>parseAndQualifyInputIdentifier</code> is used when:</p> <ul> <li><code>FlowAnalysis</code> is requested to readBatchInput and readStreamInput</li> </ul>"},{"location":"GraphIdentifierManager/#resolveDatasetReadInsideQueryDefinition","title":"resolveDatasetReadInsideQueryDefinition","text":"<pre><code>resolveDatasetReadInsideQueryDefinition(\n  context: FlowAnalysisContext,\n  rawInputName: String): DatasetIdentifier\n</code></pre> <p><code>resolveDatasetReadInsideQueryDefinition</code>...FIXME</p>"},{"location":"GraphOperations/","title":"GraphOperations","text":"<p><code>GraphOperations</code> is...FIXME</p>"},{"location":"GraphRegistrationContext/","title":"GraphRegistrationContext","text":"<p><code>GraphRegistrationContext</code> is a registry of tables, views, and flows in a pipeline (dataflow graph).</p> <p><code>GraphRegistrationContext</code> is required to create a new SqlGraphRegistrationContext.</p> <p>Eventually, <code>GraphRegistrationContext</code> becomes a DataflowGraph (to create a PipelineUpdateContextImpl to run a pipeline).</p>"},{"location":"GraphRegistrationContext/#creating-instance","title":"Creating Instance","text":"<p><code>GraphRegistrationContext</code> takes the following to be created:</p> <ul> <li> Default Catalog <li> Default Database <li> Default SQL Configuration Properties <p><code>GraphRegistrationContext</code> is created when:</p> <ul> <li><code>DataflowGraphRegistry</code> is requested to createDataflowGraph</li> </ul>"},{"location":"GraphRegistrationContext/#toDataflowGraph","title":"Create DataflowGraph","text":"<pre><code>toDataflowGraph: DataflowGraph\n</code></pre> <p><code>toDataflowGraph</code> creates a new DataflowGraph with the tables, views, sinks and flows fully-qualified, resolved, and de-duplicated.</p> AnalysisException <p><code>toDataflowGraph</code> reports an <code>AnalysisException</code> when this <code>GraphRegistrationContext</code> is empty.</p> <p><code>toDataflowGraph</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to start a pipeline run</li> </ul>"},{"location":"GraphRegistrationContext/#isPipelineEmpty","title":"isPipelineEmpty","text":"<pre><code>isPipelineEmpty: Boolean\n</code></pre> <p><code>isPipelineEmpty</code> is <code>true</code> when this pipeline (this <code>GraphRegistrationContext</code>) is empty, i.e., for all the following met:</p> <ol> <li>No tables registered</li> <li>No PersistedViews registered (among the views)</li> <li>No sinks registered</li> </ol>"},{"location":"GraphRegistrationContext/#assertNoDuplicates","title":"assertNoDuplicates","text":"<pre><code>assertNoDuplicates(\n  qualifiedTables: Seq[Table],\n  validatedViews: Seq[View],\n  qualifiedFlows: Seq[UnresolvedFlow]): Unit\n</code></pre> <p><code>assertNoDuplicates</code>...FIXME</p>"},{"location":"GraphRegistrationContext/#assertFlowIdentifierIsUnique","title":"assertFlowIdentifierIsUnique","text":"<pre><code>assertFlowIdentifierIsUnique(\n  flow: UnresolvedFlow,\n  datasetType: DatasetType,\n  flows: Seq[UnresolvedFlow]): Unit\n</code></pre> <p><code>assertFlowIdentifierIsUnique</code> throws an <code>AnalysisException</code> if the given UnresolvedFlow's identifier is used by multiple flows (among the given <code>flows</code>):</p> <pre><code>Flow [flow_name] was found in multiple datasets: [dataset_names]\n</code></pre>"},{"location":"GraphRegistrationContext/#tables","title":"Tables","text":"<p><code>GraphRegistrationContext</code> creates an empty registry of Tables when created.</p> <p>A new Table is added when <code>GraphRegistrationContext</code> is requested to register a table.</p>"},{"location":"GraphRegistrationContext/#views","title":"Views","text":"<p><code>GraphRegistrationContext</code> creates an empty registry of Views when created.</p>"},{"location":"GraphRegistrationContext/#sinks","title":"Sinks","text":"<p><code>GraphRegistrationContext</code> creates an empty registry of Sinks when created.</p> <p>A new sink is registered using registerSink (when <code>PipelinesHandler</code> is requested to define a sink).</p> <p>All the sinks registered are available via getSinks.</p> <p>A pipeline is considered empty if there are no sinks (among the other persistent entities).</p> <p>Eventually, <code>GraphRegistrationContext</code> uses the <code>sinks</code> to create a DataflowGraph.</p>"},{"location":"GraphRegistrationContext/#flows","title":"Flows","text":"<p><code>GraphRegistrationContext</code> creates an empty registry of UnresolvedFlows when created.</p>"},{"location":"GraphRegistrationContext/#registerFlow","title":"Register Flow","text":"<pre><code>registerFlow(\n  flowDef: UnresolvedFlow): Unit\n</code></pre> <p><code>registerFlow</code> adds the given UnresolvedFlow to the flows registry.</p> <p><code>registerFlow</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a flow</li> <li><code>SqlGraphRegistrationContext</code> is requested to process the following SQL queries:<ul> <li>CREATE FLOW AS INSERT INTO BY NAME</li> <li>CREATE MATERIALIZED VIEW AS</li> <li>CREATE STREAMING TABLE AS</li> <li>CREATE TEMPORARY VIEW</li> <li>CREATE VIEW</li> </ul> </li> </ul>"},{"location":"GraphRegistrationContext/#registerSink","title":"Register Sink","text":"<pre><code>registerSink(\n  sinkDef: Sink): Unit\n</code></pre> <p><code>registerSink</code> adds the given Sink to the sinks registry.</p> <p><code>registerSink</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define an output</li> </ul>"},{"location":"GraphRegistrationContext/#registerTable","title":"Register Table","text":"<pre><code>registerTable(\n  tableDef: Table): Unit\n</code></pre> <p><code>registerTable</code> adds the given Table to the tables registry.</p> <p><code>registerTable</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define an output</li> <li><code>SqlGraphRegistrationContext</code> is requested to process the following SQL queries:<ul> <li>CREATE MATERIALIZED VIEW AS</li> <li>CREATE STREAMING TABLE</li> <li>CREATE STREAMING TABLE AS</li> </ul> </li> </ul>"},{"location":"GraphValidations/","title":"GraphValidations","text":"<p><code>GraphValidations</code> is a set of validations on DataflowGraphs.</p>"},{"location":"GraphValidations/#validateTablesAreResettable","title":"validateTablesAreResettable","text":"<pre><code>validateTablesAreResettable(): Unit // (1)!\nvalidateTablesAreResettable(\n  tables: Seq[Table]): Unit\n</code></pre> <ol> <li>Validates all the tables of a DataflowGraph</li> </ol> <p><code>validateTablesAreResettable</code>...FIXME</p> <p><code>validateTablesAreResettable</code> is used when:</p> <ul> <li><code>DataflowGraph</code> is requested to validate</li> </ul>"},{"location":"Input/","title":"Input","text":"<p><code>Input</code> is an extension of the GraphElement abstraction for inputs that can load data.</p>"},{"location":"Input/#contract","title":"Contract","text":""},{"location":"Input/#load","title":"Load Data","text":"<pre><code>load(\n  readOptions: InputReadOptions): DataFrame\n</code></pre> <p>Loads a <code>DataFrame</code> with the given InputReadOptions</p> <p>See:</p> <ul> <li>ResolvedFlow</li> <li>Table</li> <li>VirtualTableInput</li> </ul> <p>Used when:</p> <ul> <li><code>FlowAnalysis</code> is requested to readGraphInput</li> </ul>"},{"location":"Input/#implementations","title":"Implementations","text":"<ul> <li>ResolvedFlow</li> <li>TableInput</li> </ul>"},{"location":"InputReadOptions/","title":"InputReadOptions","text":"<p><code>InputReadOptions</code> is...FIXME</p>"},{"location":"MaterializedView/","title":"MaterializedView","text":"<p><code>MaterializedView</code> is a <code>Table</code> that represents a materialized view in a pipeline dataflow graph.</p> <p><code>MaterializedView</code> is created using @materialized_view decorator.</p> <p><code>MaterializedView</code> is a Python class.</p>"},{"location":"MaterializedView/#materialized_view","title":"materialized_view","text":"<pre><code>materialized_view(\n    query_function: Optional[QueryFunction] = None,\n    *,\n    name: Optional[str] = None,\n    comment: Optional[str] = None,\n    spark_conf: Optional[Dict[str, str]] = None,\n    table_properties: Optional[Dict[str, str]] = None,\n    partition_cols: Optional[List[str]] = None,\n    schema: Optional[Union[StructType, str]] = None,\n    format: Optional[str] = None,\n) -&gt; Union[Callable[[QueryFunction], None], None]\n</code></pre> <p><code>materialized_view</code> uses <code>query_function</code> for the parameters unless they are specified explicitly.</p> <p><code>materialized_view</code> uses the name of the decorated function as the name of the materialized view unless specified explicitly.</p> <p><code>materialized_view</code> makes sure that GraphElementRegistry has been set (using <code>graph_element_registration_context</code> context manager).</p> Demo <pre><code>from pyspark.pipelines.graph_element_registry import (\n    graph_element_registration_context,\n    get_active_graph_element_registry,\n)\nfrom pyspark.pipelines.spark_connect_graph_element_registry import (\n    SparkConnectGraphElementRegistry,\n)\n\ndataflow_graph_id = \"demo_dataflow_graph_id\"\nregistry = SparkConnectGraphElementRegistry(spark, dataflow_graph_id)\nwith graph_element_registration_context(registry):\n    graph_registry = get_active_graph_element_registry()\n    assert graph_registry == registry\n</code></pre> <p><code>materialized_view</code> creates a new <code>MaterializedView</code> and requests the <code>GraphElementRegistry</code> to register_dataset it.</p> <p><code>materialized_view</code> creates a new <code>Flow</code> and requests the <code>GraphElementRegistry</code> to register_flow it.</p>"},{"location":"Output/","title":"Output","text":"<p><code>Output</code> is...FIXME</p>"},{"location":"PersistedView/","title":"PersistedView","text":"<p><code>PersistedView</code> is...FIXME</p>"},{"location":"PipelineEventSender/","title":"PipelineEventSender","text":"<p><code>PipelineEventSender</code> is used by PipelinesHandler to send pipelines execution progress events back to a Spark Connect client asynchronously.</p> <p><code>PipelineEventSender</code> uses spark.sql.pipelines.event.queue.capacity configuration property to control the depth of the event queue.</p>"},{"location":"PipelineEventSender/#creating-instance","title":"Creating Instance","text":"<p><code>PipelineEventSender</code> takes the following to be created:</p> <ul> <li> <code>StreamObserver[ExecutePlanResponse]</code> <li> <code>SessionHolder</code> <p><code>PipelineEventSender</code> is created when:</p> <ul> <li><code>PipelinesHandler</code> is requested to start a pipeline run</li> </ul>"},{"location":"PipelineEventSender/#queueCapacity","title":"queueCapacity","text":"<p><code>queueCapacity</code> is the value of spark.sql.pipelines.event.queue.capacity configuration property.</p> <p>Used when:</p> <ul> <li><code>PipelineEventSender</code> is requested to shouldEnqueueEvent</li> </ul>"},{"location":"PipelineEventSender/#sendEvent","title":"Send Pipeline Execution Progress Event","text":"<pre><code>sendEvent(\n  event: PipelineEvent): Unit\n</code></pre> <p><code>sendEvent</code>...FIXME</p> <p><code>sendEvent</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to start a pipeline run</li> </ul>"},{"location":"PipelineEventSender/#shouldEnqueueEvent","title":"shouldEnqueueEvent","text":"<pre><code>shouldEnqueueEvent(\n  event: PipelineEvent): Boolean\n</code></pre> <p><code>shouldEnqueueEvent</code>...FIXME</p>"},{"location":"PipelineExecution/","title":"PipelineExecution","text":"<p><code>PipelineExecution</code> manages the lifecycle of a GraphExecution (in the given PipelineUpdateContext).</p> <p><code>PipelineExecution</code> is part of PipelineUpdateContext.</p>"},{"location":"PipelineExecution/#creating-instance","title":"Creating Instance","text":"<p><code>PipelineExecution</code> takes the following to be created:</p> <ul> <li> PipelineUpdateContext <p><code>PipelineExecution</code> is created alongside PipelineUpdateContext.</p>"},{"location":"PipelineExecution/#runPipeline","title":"Run Pipeline (and Wait for Completion)","text":"<pre><code>runPipeline(): Unit\n</code></pre> <p><code>runPipeline</code> starts this pipeline and requests the PipelineExecution (of this PipelineUpdateContext) to wait for the execution to complete.</p> <p><code>runPipeline</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to start a pipeline run</li> </ul>"},{"location":"PipelineExecution/#dryRunPipeline","title":"Dry-Run Pipeline Update","text":"<pre><code>dryRunPipeline(): Unit\n</code></pre> <p><code>dryRunPipeline</code> resolves and validates this dataflow graph.</p> UnresolvedPipelineException <p>Resolving and validating this dataflow graph may fail with an <code>UnresolvedPipelineException</code>.</p> <p>If successful, <code>dryRunPipeline</code> requests this PipelineUpdateContext to emit an <code>RunCompletion</code> PipelineEvent.</p> <p><code>dryRunPipeline</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to start a dry-run pipeline update</li> </ul>"},{"location":"PipelineExecution/#startPipeline","title":"Run Pipeline Update","text":"<pre><code>startPipeline(): Unit\n</code></pre> <p><code>startPipeline</code> resolves and validates this dataflow graph.</p> <p>For a full-refresh update, <code>startPipeline</code> resets the state of all the flows in the DataflowGraph.</p> <p><code>startPipeline</code> materializes the datasets (of this dataflow graph).</p> <p><code>startPipeline</code> creates a new TriggeredGraphExecution for the materialized dataflow graph.</p> <p>In the end, <code>startPipeline</code> requests the TriggeredGraphExecution to start.</p> <p><code>startPipeline</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to runPipeline</li> </ul>"},{"location":"PipelineExecution/#awaitCompletion","title":"Await Completion","text":"<pre><code>awaitCompletion(): Unit\n</code></pre> <p><code>awaitCompletion</code> requests this GraphExecution to awaitCompletion.</p> <p><code>awaitCompletion</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to runPipeline</li> </ul>"},{"location":"PipelineExecution/#initializeGraph","title":"Initialize Dataflow Graph","text":"<pre><code>initializeGraph(): DataflowGraph\n</code></pre> <p><code>initializeGraph</code> requests this PipelineUpdateContext for the unresolved DataflowGraph to be resolved and validated.</p> <p>In the end, <code>initializeGraph</code> materializes the tables (datasets).</p> <p><code>initializeGraph</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to start the pipeline</li> </ul>"},{"location":"PipelineExecution/#graphExecution","title":"GraphExecution","text":"<pre><code>graphExecution: Option[GraphExecution]\n</code></pre> <p><code>graphExecution</code> is the GraphExecution of the pipeline.</p> <p><code>PipelineExecution</code> creates a TriggeredGraphExecution when startPipeline.</p> <p>Used in:</p> <ul> <li>awaitCompletion</li> <li>executionStarted</li> <li>startPipeline</li> <li>stopPipeline</li> </ul>"},{"location":"PipelineExecution/#executionStarted","title":"Is Execution Started","text":"<pre><code>executionStarted: Boolean\n</code></pre> <p><code>executionStarted</code> is a flag that indicates whether this GraphExecution has been created or not.</p> <p><code>executionStarted</code> is used when:</p> <ul> <li><code>SessionHolder</code> (Spark Connect) is requested to <code>removeCachedPipelineExecution</code></li> </ul>"},{"location":"PipelineExecution/#stopPipeline","title":"stopPipeline","text":"<pre><code>stopPipeline(): Unit\n</code></pre> <p><code>stopPipeline</code> requests this GraphExecution to stop.</p> <p>In case this <code>GraphExecution</code> has not been created (started) yet, <code>stopPipeline</code> reports a <code>IllegalStateException</code>:</p> <pre><code>Pipeline execution has not started yet.\n</code></pre> <p><code>stopPipeline</code> is used when:</p> <ul> <li><code>SessionHolder</code> (Spark Connect) is requested to <code>removeCachedPipelineExecution</code></li> </ul>"},{"location":"PipelineExecution/#resolveGraph","title":"Resolve and Validate Dataflow Graph","text":"<pre><code>resolveGraph(): DataflowGraph\n</code></pre> <p><code>resolveGraph</code> requests this PipelineUpdateContext for the unresolved DataflowGraph to resolve and validate.</p> UnresolvedPipelineException <p>In case of an <code>UnresolvedPipelineException</code>, <code>resolveGraph</code> handleInvalidPipeline and passes the exception along.</p> <p><code>resolveGraph</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to dry-run and run a pipeline update</li> </ul>"},{"location":"PipelineUpdateContext/","title":"PipelineUpdateContext","text":"<p><code>PipelineUpdateContext</code> is an abstraction of pipeline update contexts that can refreshTables (among other things).</p>"},{"location":"PipelineUpdateContext/#contract-subset","title":"Contract (Subset)","text":""},{"location":"PipelineUpdateContext/#fullRefreshTables","title":"fullRefreshTables","text":"<pre><code>fullRefreshTables: TableFilter\n</code></pre> <p><code>TableFilter</code> of the tables to be fully refreshed in a pipeline update run</p> <p>See:</p> <ul> <li>PipelineUpdateContextImpl</li> </ul> <p>Used when:</p> <ul> <li><code>DatasetManager</code> is requested to constructFullRefreshSet</li> <li><code>PipelineExecution</code> is requested to start a pipeline update</li> <li><code>PipelineUpdateContext</code> is requested to refreshFlows</li> <li><code>State</code> is requested to find the inputs to reset (state of)</li> </ul>"},{"location":"PipelineUpdateContext/#refreshTables","title":"refreshTables Table Filter","text":"<pre><code>refreshTables: TableFilter\n</code></pre> <p>Used when:</p> <ul> <li><code>DatasetManager</code> is requested to constructFullRefreshSet</li> <li><code>PipelineUpdateContext</code> is requested to refreshFlows</li> </ul>"},{"location":"PipelineUpdateContext/#storageRoot","title":"Storage Root","text":"<pre><code>storageRoot: String\n</code></pre> <p>The root storage location of pipeline metadata (e.g., checkpoints of streaming flows)</p> <p>Used when:</p> <ul> <li><code>FlowSystemMetadata</code> is requested to flowCheckpointsDirOpt</li> </ul>"},{"location":"PipelineUpdateContext/#unresolvedGraph","title":"Unresolved Dataflow Graph","text":"<pre><code>unresolvedGraph: DataflowGraph\n</code></pre> <p>The unresolved DataflowGraph of this pipeline update (pipeline run) to execute</p> <p>Used when:</p> <ul> <li><code>PipelineExecution</code> is requested to resolve this unresolved DataflowGraph</li> </ul>"},{"location":"PipelineUpdateContext/#implementations","title":"Implementations","text":"<ul> <li>PipelineUpdateContextImpl</li> </ul>"},{"location":"PipelineUpdateContext/#pipelineExecution","title":"PipelineExecution","text":"<pre><code>pipelineExecution: PipelineExecution\n</code></pre> <p><code>PipelineUpdateContext</code> creates a PipelineExecution when created.</p> <p>The <code>PipelineExecution</code> is created for this <code>PipelineUpdateContext</code>.</p>"},{"location":"PipelineUpdateContext/#refreshFlows","title":"refreshFlows","text":"<pre><code>refreshFlows: FlowFilter\n</code></pre> Final Method <p><code>refreshFlows</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p> <p><code>refreshFlows</code>...FIXME</p> <p><code>refreshFlows</code> is used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to start</li> </ul>"},{"location":"PipelineUpdateContext/#initializeGraph","title":"Initialize Dataflow Graph","text":"<pre><code>initializeGraph(): DataflowGraph\n</code></pre> <p><code>initializeGraph</code>...FIXME</p> <p><code>initializeGraph</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to start the pipeline</li> </ul>"},{"location":"PipelineUpdateContextImpl/","title":"PipelineUpdateContextImpl","text":"<p><code>PipelineUpdateContextImpl</code> is a PipelineUpdateContext.</p>"},{"location":"PipelineUpdateContextImpl/#creating-instance","title":"Creating Instance","text":"<p><code>PipelineUpdateContextImpl</code> takes the following to be created:</p> <ul> <li> DataflowGraph <li> <code>PipelineEvent</code> Callback (<code>PipelineEvent =&gt; Unit</code>) <li> <code>TableFilter</code> of the tables to be refreshed (default: <code>AllTables</code>) <li> <code>TableFilter</code> of the tables to be refreshed (default: <code>NoTables</code>) <li> Storage root <p>While being created, <code>PipelineUpdateContextImpl</code> validates the storage root.</p> <p><code>PipelineUpdateContextImpl</code> is created when:</p> <ul> <li><code>PipelinesHandler</code> is requested to run a pipeline</li> </ul>"},{"location":"PipelineUpdateContextImpl/#validateStorageRoot","title":"Validate Storage Root","text":"<pre><code>validateStorageRoot(\n  storageRoot: String): Unit\n</code></pre> <p><code>validateStorageRoot</code> asserts that the given <code>storageRoot</code> meets the following requirements:</p> <ol> <li>It is an absolute path</li> <li>The schema is defined</li> </ol> <p>Otherwise, <code>validateStorageRoot</code> reports a <code>SparkException</code>:</p> <pre><code>Pipeline storage root must be an absolute path with a URI scheme (e.g., file://, s3a://, hdfs://).\nGot: `[storage_root]`.\n</code></pre>"},{"location":"PipelinesErrors/","title":"PipelinesErrors","text":"<p><code>PipelinesErrors</code> is...FIXME</p>"},{"location":"PipelinesHandler/","title":"PipelinesHandler","text":"<p><code>PipelinesHandler</code> is used to handle pipeline commands in Spark Connect (SparkConnectPlanner, precisely).</p> <p><code>PipelinesHandler</code> acts as a bridge between Python execution environment of Spark Declarative Pipelines and Spark Connect Server (where pipeline execution happens).</p> <p></p>"},{"location":"PipelinesHandler/#handlePipelinesCommand","title":"Handle Pipelines Command","text":"<pre><code>handlePipelinesCommand(\n  sessionHolder: SessionHolder,\n  cmd: proto.PipelineCommand,\n  responseObserver: StreamObserver[ExecutePlanResponse],\n  transformRelationFunc: Relation =&gt; LogicalPlan): PipelineCommandResult\n</code></pre> <p><code>handlePipelinesCommand</code> handles the given pipeline <code>cmd</code> command.</p> PipelineCommand Description Initiator <code>CREATE_DATAFLOW_GRAPH</code> Creates a new dataflow graph pyspark.pipelines.spark_connect_pipeline <code>DROP_DATAFLOW_GRAPH</code> Drops a pipeline <code>DEFINE_OUTPUT</code> Defines an output (a table, a materialized view, a temporary view or a sink) SparkConnectGraphElementRegistry <code>DEFINE_FLOW</code> Defines a flow SparkConnectGraphElementRegistry <code>START_RUN</code> Starts a pipeline run pyspark.pipelines.spark_connect_pipeline <code>DEFINE_SQL_GRAPH_ELEMENTS</code> DEFINE_SQL_GRAPH_ELEMENTS SparkConnectGraphElementRegistry UnsupportedOperationException <p><code>handlePipelinesCommand</code> reports an <code>UnsupportedOperationException</code> for incorrect commands:</p> <pre><code>[other] not supported\n</code></pre> <p><code>handlePipelinesCommand</code> is used when:</p> <ul> <li><code>SparkConnectPlanner</code> (Spark Connect) is requested to <code>handlePipelineCommand</code> (for <code>PIPELINE_COMMAND</code> command)</li> </ul>"},{"location":"PipelinesHandler/#CREATE_DATAFLOW_GRAPH","title":"CREATE_DATAFLOW_GRAPH <p>handlePipelinesCommand creates a dataflow graph and sends the graph ID back.</p>","text":""},{"location":"PipelinesHandler/#DROP_DATAFLOW_GRAPH","title":"DROP_DATAFLOW_GRAPH <p>handlePipelinesCommand...FIXME</p>","text":""},{"location":"PipelinesHandler/#DEFINE_OUTPUT","title":"DEFINE_OUTPUT <p>handlePipelinesCommand prints out the following INFO message to the logs:</p> <pre><code>Define pipelines output cmd received: [cmd]\n</code></pre> <p><code>handlePipelinesCommand</code> defines an output and responds with a resolved dataset (with a catalog and a database when specified)</p>","text":""},{"location":"PipelinesHandler/#DEFINE_FLOW","title":"DEFINE_FLOW <p>handlePipelinesCommand prints out the following INFO message to the logs:</p> <pre><code>Define pipelines flow cmd received: [cmd]\n</code></pre> <p><code>handlePipelinesCommand</code> defines a flow.</p>","text":""},{"location":"PipelinesHandler/#START_RUN","title":"START_RUN <p>handlePipelinesCommand prints out the following INFO message to the logs:</p> <pre><code>Start pipeline cmd received: [cmd]\n</code></pre> <p><code>handlePipelinesCommand</code> starts a pipeline run.</p>","text":""},{"location":"PipelinesHandler/#DEFINE_SQL_GRAPH_ELEMENTS","title":"DEFINE_SQL_GRAPH_ELEMENTS <p>handlePipelinesCommand...FIXME</p>","text":""},{"location":"PipelinesHandler/#startRun","title":"Start Pipeline Run","text":"<pre><code>startRun(\n  cmd: proto.PipelineCommand.StartRun,\n  responseObserver: StreamObserver[ExecutePlanResponse],\n  sessionHolder: SessionHolder): Unit\n</code></pre> START_RUN Pipeline Command <p><code>startRun</code> is used to handle START_RUN pipeline command.</p> <p><code>startRun</code> finds the GraphRegistrationContext by <code>dataflowGraphId</code> in the DataflowGraphRegistry (in the given <code>SessionHolder</code>).</p> <p><code>startRun</code> creates a PipelineEventSender to send pipeline execution progress events back to the Spark Connect client (Python pipeline runtime).</p> <p><code>startRun</code> creates a PipelineUpdateContextImpl (with the <code>PipelineEventSender</code>).</p> <p>In the end, <code>startRun</code> requests the <code>PipelineUpdateContextImpl</code> for the PipelineExecution to run a pipeline or dry-run a pipeline for <code>dry-run</code> or <code>run</code> command, respectively.</p>"},{"location":"PipelinesHandler/#createDataflowGraph","title":"Create Dataflow Graph","text":"<pre><code>createDataflowGraph(\n  cmd: proto.PipelineCommand.CreateDataflowGraph,\n  spark: SparkSession): String\n</code></pre> CREATE_DATAFLOW_GRAPH Pipeline Command <p><code>createDataflowGraph</code> is used to handle CREATE_DATAFLOW_GRAPH pipeline command.</p> <p><code>createDataflowGraph</code> gets the catalog (from the given <code>CreateDataflowGraph</code> if defined in the pipeline specification file) or prints out the following INFO message to the logs and uses the current catalog instead.</p> <pre><code>No default catalog was supplied. Falling back to the current catalog: [currentCatalog].\n</code></pre> <p><code>createDataflowGraph</code> gets the database (from the given <code>CreateDataflowGraph</code> if defined in the pipeline specification file) or prints out the following INFO message to the logs and uses the current database instead.</p> <pre><code>No default database was supplied. Falling back to the current database: [currentDatabase].\n</code></pre> <p>In the end, <code>createDataflowGraph</code> creates a dataflow graph (in the session's DataflowGraphRegistry).</p>"},{"location":"PipelinesHandler/#defineSqlGraphElements","title":"Define SQL Datasets","text":"<pre><code>defineSqlGraphElements(\n  cmd: proto.PipelineCommand.DefineSqlGraphElements,\n  session: SparkSession): Unit\n</code></pre> DEFINE_SQL_GRAPH_ELEMENTS Pipeline Command <p><code>defineSqlGraphElements</code> is used to handle DEFINE_SQL_GRAPH_ELEMENTS pipeline command.</p> <p><code>defineSqlGraphElements</code> looks up the GraphRegistrationContext for the dataflow graph ID (from the given <code>DefineSqlGraphElements</code> command and in the given <code>SessionHolder</code>).</p> <p><code>defineSqlGraphElements</code> creates a new SqlGraphRegistrationContext (for the <code>GraphRegistrationContext</code>) to process the SQL definition file.</p>"},{"location":"PipelinesHandler/#defineOutput","title":"Define Output","text":"<pre><code>defineOutput(\n  output: proto.PipelineCommand.DefineOutput,\n  sessionHolder: SessionHolder): TableIdentifier\n</code></pre> DEFINE_OUTPUT Pipeline Command <p><code>defineOutput</code> is used to handle DEFINE_OUTPUT pipeline command.</p> <p><code>defineOutput</code> looks up the GraphRegistrationContext for the dataflow graph ID of the given <code>output</code> (or throws a <code>SparkException</code> if not found).</p> <p><code>defineOutput</code> branches off based on the <code>output</code> type:</p> Dataset Type Action <code>MATERIALIZED_VIEW</code> or <code>TABLE</code> Registers a table <code>TEMPORARY_VIEW</code> Registers a view <code>SINK</code> Registers a sink IllegalArgumentException <p>For unknown types, <code>defineOutput</code> reports an <code>IllegalArgumentException</code>:</p> <pre><code>Unknown output type: [type]\n</code></pre>"},{"location":"PipelinesHandler/#defineFlow","title":"Define Flow","text":"<pre><code>defineFlow(\n  flow: proto.PipelineCommand.DefineFlow,\n  transformRelationFunc: Relation =&gt; LogicalPlan,\n  sessionHolder: SessionHolder): TableIdentifier\n</code></pre> DEFINE_FLOW Pipeline Command <p><code>defineFlow</code> is used to handle DEFINE_FLOW pipeline command.</p> <p><code>defineFlow</code> looks up the GraphRegistrationContext for the given <code>flow</code> (or throws a <code>SparkException</code> if not found).</p> Implicit Flows <p>Implicit Flows are flows with the name of the target datasets (i.e. one defined as part of dataset creation).</p> <p>Implicit flows can be defined with multi-part identifiers (as the corresponding datasets).</p> <p>Multi-part identifiers are composed of catalog, schema and table parts (separated by <code>.</code> (dot)).</p> <p><code>defineFlow</code> creates a flow identifier (for the <code>flow</code> name).</p> AnalysisExceptions <p><code>defineFlow</code> reports an <code>AnalysisException</code> for the following:</p> <ol> <li><code>DefineFlow</code> proto command defines the flow as a one-time flow</li> <li>The given <code>flow</code> is not an implicit flow, but is defined with a multi-part identifier.</li> </ol> <p>In the end, <code>defineFlow</code> registers a flow (with a proper FlowFunction).</p>"},{"location":"PipelinesTableProperties/","title":"PipelinesTableProperties","text":"<p><code>PipelinesTableProperties</code> defines the supported pipelines table properties.</p> <p>Pipelines table properties start with <code>pipelines.</code> prefix.</p>"},{"location":"PipelinesTableProperties/#pipelines.reset.allowed","title":"pipelines.reset.allowed <p>Controls whether or not a table should be reset when a Reset is triggered.</p> <p>default: <code>true</code></p> <p>Used when:</p> <ul> <li><code>DatasetManager</code> is requested to constructFullRefreshSet</li> <li><code>State</code> is requested to findElementsToReset</li> <li><code>GraphValidations</code> is requested to validate that tables are allowed to be reset</li> </ul>","text":""},{"location":"ResolutionCompletedFlow/","title":"ResolutionCompletedFlow","text":"<p><code>ResolutionCompletedFlow</code> is an extension of the Flow abstraction for flows with a flow function executed successfully or not (ResolvedFlow and ResolutionFailedFlow, respectively).</p>"},{"location":"ResolutionCompletedFlow/#contract","title":"Contract","text":""},{"location":"ResolutionCompletedFlow/#flow","title":"UnresolvedFlow","text":"<pre><code>flow: UnresolvedFlow\n</code></pre> <p>UnresolvedFlow</p> <p>Used when:</p> <ul> <li><code>CoreDataflowNodeProcessor</code> is requested to processNode</li> <li><code>DataflowGraph</code> is requested to reanalyzeFlow</li> <li>This <code>ResolutionCompletedFlow</code> is requested for the destination identifier, FlowFunction, identifier, QueryOrigin, QueryContext</li> </ul>"},{"location":"ResolutionCompletedFlow/#funcResult","title":"FlowFunctionResult","text":"<pre><code>funcResult: FlowFunctionResult\n</code></pre> <p>FlowFunctionResult</p> <p>FlowFunctionResult</p> <p><code>FlowFunctionResult</code> is used to assert that ResolvedFlow and ResolutionFailedFlow are in proper state.</p> <p>Used when:</p> <ul> <li>This <code>ResolutionCompletedFlow</code> is requested for the sqlConf</li> <li><code>ResolutionFailedFlow</code> is requested for the failure</li> <li><code>ResolvedFlow</code> is requested for the logical query plan and inputs</li> <li><code>GraphValidations</code> is requested to validatePersistedViewSources, validateSuccessfulFlowAnalysis</li> </ul>"},{"location":"ResolutionCompletedFlow/#implementations","title":"Implementations","text":"<ul> <li>ResolutionFailedFlow</li> <li>ResolvedFlow</li> </ul>"},{"location":"ResolutionFailedFlow/","title":"ResolutionFailedFlow","text":"<p><code>ResolutionFailedFlow</code> is a ResolutionCompletedFlow.</p>"},{"location":"ResolutionFailedFlow/#creating-instance","title":"Creating Instance","text":"<p><code>ResolutionFailedFlow</code> takes the following to be created:</p> <ul> <li> UnresolvedFlow <li> FlowFunctionResult <p><code>ResolutionFailedFlow</code> is created when:</p> <ul> <li><code>FlowResolver</code> is requested to attemptResolveFlow</li> </ul>"},{"location":"ResolvedFlow/","title":"ResolvedFlow","text":"<p><code>ResolvedFlow</code> is an extension of the ResolutionCompletedFlow and Input abstractions for resolved flows that are resolved successfully.</p> <p><code>ResolvedFlow</code>s are Flows with FlowFunctionResult being resolved.</p> <p><code>ResolvedFlow</code> is a mere wrapper around FlowFunctionResult.</p>"},{"location":"ResolvedFlow/#implementations","title":"Implementations","text":"<ul> <li>AppendOnceFlow</li> <li>CompleteFlow</li> <li>StreamingFlow</li> </ul>"},{"location":"ResolvedFlow/#inputs","title":"Inputs","text":"<pre><code>inputs: Set[TableIdentifier]\n</code></pre> <p><code>inputs</code> requests this FlowFunctionResult for the inputs.</p> <p><code>inputs</code> is used when:</p> <ul> <li><code>DatasetManager</code> is requested to materializeViews</li> <li><code>GraphOperations</code> is requested for the flowNodes</li> <li><code>GraphValidations</code> is requested to validateGraphIsTopologicallySorted</li> </ul>"},{"location":"SQLConf/","title":"SQLConf","text":"<p><code>SQLConf</code> is an internal configuration store of the configuration properties and hints used in Spark SQL.</p>"},{"location":"SQLConf/#sparksqlpipelines","title":"spark.sql.pipelines","text":""},{"location":"SQLConf/#streamStatePollingInterval","title":"streamStatePollingInterval <p>spark.sql.pipelines.execution.streamstate.pollingInterval</p>","text":""},{"location":"SQLConf/#watchdogMinRetryTimeInSeconds","title":"watchdogMinRetryTimeInSeconds <p>spark.sql.pipelines.execution.watchdog.minRetryTime</p>","text":""},{"location":"SQLConf/#watchdogMaxRetryTimeInSeconds","title":"watchdogMaxRetryTimeInSeconds <p>spark.sql.pipelines.execution.watchdog.maxRetryTime</p>","text":""},{"location":"SQLConf/#maxConcurrentFlows","title":"maxConcurrentFlows <p>spark.sql.pipelines.execution.maxConcurrentFlows</p>","text":""},{"location":"SQLConf/#timeoutMsForTerminationJoinAndLock","title":"timeoutMsForTerminationJoinAndLock <p>spark.sql.pipelines.timeoutMsForTerminationJoinAndLock</p>","text":""},{"location":"SQLConf/#maxFlowRetryAttempts","title":"maxFlowRetryAttempts <p>spark.sql.pipelines.maxFlowRetryAttempts</p>","text":""},{"location":"SQLConf/#queueCapacity","title":"queueCapacity <p>spark.sql.pipelines.event.queue.capacity</p>","text":""},{"location":"SchemaInferenceUtils/","title":"SchemaInferenceUtils","text":"<p><code>SchemaInferenceUtils</code> is...FIXME</p>"},{"location":"Sink/","title":"Sink","text":"<p><code>Sink</code> is an extension of the GraphElement and Output abstractions for pipeline sinks that can define their write format and options.</p> <p>A sink is a generic target for a flow to send data that is external to a pipeline.</p> <p>Sinks are not registered in a Spark catalog.</p>"},{"location":"Sink/#contract","title":"Contract","text":""},{"location":"Sink/#format","title":"Format","text":"<pre><code>format: String\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a sink (output)</li> <li><code>SinkWrite</code> is requested to start a stream</li> </ul>"},{"location":"Sink/#options","title":"Options","text":"<pre><code>options: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a sink (output)</li> <li><code>SinkWrite</code> is requested to start a stream</li> </ul>"},{"location":"Sink/#implementations","title":"Implementations","text":"<ul> <li>SinkImpl</li> </ul>"},{"location":"SinkImpl/","title":"SinkImpl","text":"<p><code>SinkImpl</code> is a Sink.</p>"},{"location":"SinkImpl/#creating-instance","title":"Creating Instance","text":"<p><code>SinkImpl</code> takes the following to be created:</p> <ul> <li> TableIdentifier <li> Format <li> Options <li> QueryOrigin <p><code>SinkImpl</code> is created when:</p> <ul> <li><code>PipelinesHandler</code> is requested to defineOutput</li> </ul>"},{"location":"SinkWrite/","title":"SinkWrite Flow Execution","text":"<p><code>SinkWrite</code> is a StreamingFlowExecution that writes a streaming <code>DataFrame</code> to a Sink.</p> <p><code>SinkWrite</code> represents a StreamingFlow with a Sink as the output destination at execution.</p> <p>When executed, <code>SinkWrite</code> starts a streaming query to append new rows to an output table.</p>"},{"location":"SinkWrite/#creating-instance","title":"Creating Instance","text":"<p><code>SinkWrite</code> takes the following to be created:</p> <ul> <li> TableIdentifier <li> ResolvedFlow <li> DataflowGraph <li> PipelineUpdateContext <li> Checkpoint Location <li> Streaming Trigger <li> Destination (Sink) <li> SQL Configuration <p><code>SinkWrite</code> is created when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a ResolvedFlow</li> </ul>"},{"location":"SinkWrite/#startStream","title":"Start Streaming Query","text":"StreamingFlowExecution <pre><code>startStream(): StreamingQuery\n</code></pre> <p><code>startStream</code> is part of the StreamingFlowExecution abstraction.</p> <p><code>startStream</code> builds the logical query plan of this flow's structured query (requesting the DataflowGraph to reanalyze this flow).</p> <p><code>startStream</code> creates a <code>DataStreamWriter</code> (Spark Structured Streaming) with the following:</p> <code>DataStreamWriter</code>'s Property Value <code>queryName</code> This displayName <code>checkpointLocation</code> option This checkpoint path <code>trigger</code> This streaming trigger <code>outputMode</code> Append (always) <code>format</code> The format of this output sink <code>options</code> The options of this output sink <p>In the end, <code>startStream</code> starts the streaming write query to this output table.</p>"},{"location":"SparkConnectGraphElementRegistry/","title":"SparkConnectGraphElementRegistry","text":"<p><code>SparkConnectGraphElementRegistry</code> is a GraphElementRegistry.</p> <p><code>SparkConnectGraphElementRegistry</code> acts as a communication bridge between Spark Declarative Pipelines Python execution environment and Spark Connect Server (with PipelinesHandler).</p>"},{"location":"SparkConnectGraphElementRegistry/#creating-instance","title":"Creating Instance","text":"<p><code>SparkConnectGraphElementRegistry</code> takes the following to be created:</p> <ul> <li> <code>SparkSession</code> (<code>SparkConnectClient</code>) <li> Dataflow Graph ID <p><code>SparkConnectGraphElementRegistry</code> is created when:</p> <ul> <li><code>pyspark.pipelines.cli</code> is requested to run</li> </ul>"},{"location":"SparkConnectGraphElementRegistry/#register_flow","title":"Register Flow","text":"GraphElementRegistry <pre><code>register_flow(\n    self,\n    flow: Flow\n) -&gt; None\n</code></pre> <p><code>register_flow</code> is part of the GraphElementRegistry abstraction.</p> <p><code>register_flow</code> requests this SparkConnectClient to execute a <code>PipelineCommand.DefineFlow</code> command.</p> PipelinesHandler on Spark Connect Server <p><code>DefineFlow</code> commands are handled by PipelinesHandler on Spark Connect Server.</p>"},{"location":"SparkConnectGraphElementRegistry/#register_output","title":"Register Output","text":"GraphElementRegistry <pre><code>register_output(\n    self,\n    output: Output\n) -&gt; None\n</code></pre> <p><code>register_output</code> is part of the GraphElementRegistry abstraction.</p> <p><code>register_output</code> requests this SparkConnectClient to execute a <code>DefineOutput</code> pipeline command.</p> PipelinesHandler on Spark Connect Server <p><code>DefineOutput</code> command is handled by PipelinesHandler on Spark Connect Server.</p>"},{"location":"SparkPipelines/","title":"SparkPipelines \u2014 Spark Pipelines CLI","text":"<p><code>SparkPipelines</code> is a standalone application that is executed using spark-pipelines shell script.</p> <p><code>SparkPipelines</code> is a Scala \"launchpad\" to execute pyspark/pipelines/cli.py Python script (through SparkSubmit).</p>"},{"location":"SparkPipelines/#pyspark-pipelines-cli","title":"cli.py","text":"<p><code>pyspark/pipelines/cli.py</code> is the heart of the Spark Pipelines CLI (launched using spark-pipelines shell script).</p> <p>As a Python script, <code>cli.py</code> can simply import Python libraries (to trigger their execution) whereas SQL libraries are left untouched and sent over the wire to a Spark Connect server (PipelinesHandler) for execution.</p> <p>The Pipelines CLI supports the following commands:</p> <ul> <li>dry-run</li> <li>init</li> <li>run</li> </ul> uv run <pre><code>$ pwd\n/Users/jacek/oss/spark/python\n\n$ PYTHONPATH=. uv run \\\n    --with grpcio-status \\\n    --with grpcio \\\n    --with pyarrow \\\n    --with pandas \\\n    --with pyspark \\\n    python pyspark/pipelines/cli.py\n...\nusage: cli.py [-h] {run,dry-run,init} ...\ncli.py: error: the following arguments are required: command\n</code></pre>"},{"location":"SparkPipelines/#dry-run","title":"dry-run","text":"<p>Launch a run that just validates the graph and checks for errors</p> Option Description Default <code>--spec</code> Path to the pipeline spec (undefined)"},{"location":"SparkPipelines/#init","title":"init","text":"<p>Generate a sample pipeline project, including a spec file and example definitions</p> Option Description Default Required <code>--name</code> Name of the project. A directory with this name will be created underneath the current directory (undefined) \u2705 <pre><code>$ ./bin/spark-pipelines init --name hello-pipelines\nPipeline project 'hello-pipelines' created successfully. To run your pipeline:\ncd 'hello-pipelines'\nspark-pipelines run\n</code></pre>"},{"location":"SparkPipelines/#run","title":"run","text":"<p>Run a pipeline. If no <code>--refresh</code> option specified, a default incremental update is performed.</p> Option Description Default <code>--spec</code> Path to the pipeline spec (undefined) <code>--full-refresh</code> List of datasets to reset and recompute (comma-separated) (empty) <code>--full-refresh-all</code> Perform a full graph reset and recompute (undefined) <code>--refresh</code> List of datasets to update (comma-separated) (empty) <p>When executed, <code>run</code> prints out the following log message:</p> <pre><code>Loading pipeline spec from [spec_path]...\n</code></pre> <p><code>run</code> loads a pipeline spec.</p> <p><code>run</code> prints out the following log message:</p> <pre><code>Creating Spark session...\n</code></pre> <p><code>run</code> creates a Spark session with the configurations from the pipeline spec.</p> <p><code>run</code> prints out the following log message:</p> <pre><code>Creating dataflow graph...\n</code></pre> <p><code>run</code> sends a <code>CreateDataflowGraph</code> command for execution in the Spark Connect server.</p> <p>Spark Connect Server and Command Execution</p> <p><code>CreateDataflowGraph</code> is handled by PipelinesHandler on the Spark Connect Server.</p> <p><code>run</code> prints out the following log message:</p> <pre><code>Dataflow graph created (ID: [dataflow_graph_id]).\n</code></pre> <p><code>run</code> prints out the following log message:</p> <pre><code>Registering graph elements...\n</code></pre> <p><code>run</code> creates a SparkConnectGraphElementRegistry and <code>register_definitions</code>.</p> <p><code>run</code> prints out the following log message:</p> <pre><code>Starting run (dry=[dry], full_refresh=[full_refresh], full_refresh_all=[full_refresh_all], refresh=[refresh])...\n</code></pre> <p><code>run</code> sends a <code>StartRun</code> command for execution in the Spark Connect Server.</p> <p>StartRun Command and PipelinesHandler</p> <p><code>StartRun</code> command is handled by PipelinesHandler on the Spark Connect Server.</p> <p>In the end, <code>run</code> keeps printing out pipeline events from the Spark Connect server.</p>"},{"location":"SqlGraphRegistrationContext/","title":"SqlGraphRegistrationContext","text":""},{"location":"SqlGraphRegistrationContext/#creating-instance","title":"Creating Instance","text":"<p><code>SqlGraphRegistrationContext</code> takes the following to be created:</p> <ul> <li> GraphRegistrationContext <p><code>SqlGraphRegistrationContext</code> is created when:</p> <ul> <li><code>PipelinesHandler</code> is requested to handle DEFINE_SQL_GRAPH_ELEMENTS command (and defineSqlGraphElements)</li> <li><code>SqlGraphRegistrationContext</code> is requested to process a SQL file</li> </ul>"},{"location":"SqlGraphRegistrationContext/#context","title":"SqlGraphRegistrationContextState","text":"<p>When created, <code>SqlGraphRegistrationContext</code> creates a SqlGraphRegistrationContextState (with the defaultCatalog, the defaultDatabase and the defaultSqlConf).</p>"},{"location":"SqlGraphRegistrationContext/#processSqlFile","title":"Process SQL File","text":"<pre><code>processSqlFile(\n  sqlText: String,\n  sqlFilePath: String,\n  spark: SparkSession): Unit\n</code></pre> <p><code>processSqlFile</code> creates a SqlGraphRegistrationContext for this GraphRegistrationContext.</p> <p>Warning</p> <p>Why does <code>processSqlFile</code> creates a brand new SqlGraphRegistrationContext for the same GraphRegistrationContext it is executed with?!</p> <p><code>processSqlFile</code> splits the contents of the SQL file into separate queries and processes every SQL query.</p> <p><code>processSqlFile</code> is used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to defineSqlGraphElements</li> </ul>"},{"location":"SqlGraphRegistrationContext/#processSqlQuery","title":"Process Single SQL Query","text":"<pre><code>processSqlQuery(\n  queryPlan: LogicalPlan,\n  queryOrigin: QueryOrigin): Unit\n</code></pre> <p><code>processSqlQuery</code> handles (processes) the given LogicalPlan logical commands:</p> Logical Command Command Handler Datasets CreateFlowCommand CreateFlowHandler UnresolvedFlow (once disabled) CreateMaterializedViewAsSelect CreateMaterializedViewAsSelectHandler Table (isStreamingTable disabled)UnresolvedFlow (once disabled) CreateStreamingTable CreateStreamingTableHandler Table (isStreamingTable enabled) CreateStreamingTableAsSelect CreateStreamingTableAsSelectHandler Table (isStreamingTable enabled)UnresolvedFlow (once disabled) CreateView CreatePersistedViewCommandHandler PersistedViewUnresolvedFlow (once disabled) CreateViewCommand CreateTemporaryViewHandler TemporaryViewUnresolvedFlow (once disabled) SetCatalogCommand SetCatalogCommandHandler SetCommand SetCommandHandler SetNamespaceCommand SetNamespaceCommandHandler"},{"location":"SqlGraphRegistrationContext/#splitSqlFileIntoQueries","title":"splitSqlFileIntoQueries","text":"<pre><code>splitSqlFileIntoQueries(\n  spark: SparkSession,\n  sqlFileText: String,\n  sqlFilePath: String): Seq[SqlQueryPlanWithOrigin]\n</code></pre> <p><code>splitSqlFileIntoQueries</code>...FIXME</p>"},{"location":"SqlGraphRegistrationContext/#logical-command-handlers","title":"Logical Command Handlers","text":""},{"location":"SqlGraphRegistrationContext/#CreateFlowCommand","title":"CreateFlowCommand","text":"<p>CreateFlowCommand logical commands are handled by <code>CreateFlowHandler</code>.</p> <p>A flow name must be a single-part name (that is resolved against the current pipelines catalog and database).</p> <p>The flowOperation of a CreateFlowCommand command must be InsertIntoStatement.</p> Warning <p>Only <code>INSERT INTO ... BY NAME</code> flows are supported in Spark Declarative Pipelines.</p> <p><code>INSERT OVERWRITE</code> flows are not supported.</p> <p><code>IF NOT EXISTS</code> not supported for flows.</p> <p>Neither partition spec nor user-specified schema can be specified.</p> <p>In the end, <code>CreateFlowHandler</code> requests this GraphRegistrationContext to register an UnresolvedFlow.</p>"},{"location":"SqlGraphRegistrationContext/#CreateMaterializedViewAsSelect","title":"CreateMaterializedViewAsSelect","text":"<p>processSqlQuery handles CreateMaterializedViewAsSelect logical commands using <code>CreateMaterializedViewAsSelectHandler</code>.</p> <p><code>CreateMaterializedViewAsSelectHandler</code> requests this GraphRegistrationContext to register a table and a flow (that backs the materialized view).</p>"},{"location":"SqlGraphRegistrationContext/#CreateStreamingTable","title":"CreateStreamingTable","text":"<p>processSqlQuery handles CreateStreamingTable logical commands using <code>CreateStreamingTableHandler</code>.</p> <p><code>CreateStreamingTableHandler</code> requests this SqlGraphRegistrationContextState to register a streaming table.</p>"},{"location":"SqlGraphRegistrationContext/#CreateStreamingTableAsSelect","title":"CreateStreamingTableAsSelect","text":"<p>processSqlQuery handles CreateStreamingTableAsSelect logical commands using <code>CreateStreamingTableAsSelectHandler</code>.</p> <p><code>CreateStreamingTableAsSelectHandler</code> requests this SqlGraphRegistrationContextState to register a streaming table and the accompanying flow (for the streaming table).</p>"},{"location":"SqlGraphRegistrationContext/#CreateView","title":"CreateView","text":"<p>processSqlQuery handles CreateView logical commands using <code>CreatePersistedViewCommandHandler</code>.</p> <p><code>CreatePersistedViewCommandHandler</code> requests this GraphRegistrationContext to register a PersistedView and the accompanying flow (for the <code>PersistedView</code>).</p>"},{"location":"SqlGraphRegistrationContext/#CreateViewCommand","title":"CreateViewCommand","text":"<p>processSqlQuery handles CreateViewCommand logical commands using <code>CreateTemporaryViewHandler</code>.</p> <p><code>CreateTemporaryViewHandler</code> requests this GraphRegistrationContext to register a TemporaryView and the accompanying flow (for the <code>TemporaryView</code>).</p>"},{"location":"SqlGraphRegistrationContext/#SetCatalogCommand","title":"SetCatalogCommand","text":"<p>processSqlQuery handles SetCatalogCommand logical commands using <code>SetCatalogCommandHandler</code>.</p> <p><code>SetCatalogCommandHandler</code> requests this SqlGraphRegistrationContextState to setCurrentCatalog to the catalogName of the given SetCatalogCommand.</p> <p>In the end, <code>SetCatalogCommandHandler</code> requests this SqlGraphRegistrationContextState to clearCurrentDatabase.</p>"},{"location":"SqlGraphRegistrationContext/#SetCommand","title":"SetCommand","text":"<p>processSqlQuery handles SetCommand logical commands using <code>SetCommandHandler</code>.</p> <p><code>SetCommandHandler</code> requests this SqlGraphRegistrationContextState to setSqlConf with the key-value pair of the given SetCommand logical command.</p> RuntimeException <p><code>handle</code> makes sure that the given <code>SetCommand</code> comes with a <code>key = value</code> pair or throws a <code>RuntimeException</code>:</p> <pre><code>Invalid SET command without key-value pair\n</code></pre> <pre><code>Invalid SET command without value\n</code></pre>"},{"location":"SqlGraphRegistrationContext/#SetNamespaceCommand","title":"SetNamespaceCommand","text":"<p>processSqlQuery handles SetNamespaceCommand logical commands using <code>SetNamespaceCommandHandler</code>.</p> <p><code>SetNamespaceCommandHandler</code> requests this SqlGraphRegistrationContextState for the following:</p> <ul> <li>For a <code>database</code>-only, single-part namespace, setCurrentDatabase</li> <li>For a <code>catalog.database</code> two-part namespace, setCurrentCatalog and setCurrentDatabase</li> </ul> SparkException <p><code>handle</code> throws a <code>SparkException</code> for invalid namespaces:</p> <pre><code>Invalid schema identifier provided on USE command: [namespace]\n</code></pre>"},{"location":"SqlGraphRegistrationContextState/","title":"SqlGraphRegistrationContextState","text":"<p><code>SqlGraphRegistrationContextState</code> is...FIXME</p>"},{"location":"State/","title":"State","text":""},{"location":"State/#reset","title":"Reset State of All Flows","text":"<pre><code>reset(\n  resolvedGraph: DataflowGraph,\n  env: PipelineUpdateContext): Seq[Input]\nreset(\n  flow: ResolvedFlow,\n  env: PipelineUpdateContext,\n  graph: DataflowGraph): Unit // (1)!\n</code></pre> <ol> <li>A private method</li> </ol> <p><code>reset</code> finds ResolvedFlows to reset in the given DataflowGraph (and the PipelineUpdateContext).</p> <p>Info</p> <p><code>reset</code> handles ResolvedFlows only.</p> <p><code>reset</code> prints out the following INFO message to the logs:</p> <pre><code>Clearing out state for flow [displayName]\n</code></pre> <p><code>reset</code> creates a FlowSystemMetadata.</p> <p>For no checkpoint directory available for the ResolvedFlow, <code>reset</code> prints out the following INFO message to the logs and exits.</p> <pre><code>Skipping resetting flow [identifier]\nsince its destination not been previously materialized\nand we can't find the checkpoint location.\n</code></pre> <p>Otherwise, when there is a checkpoint directory available, <code>reset</code> creates a new checkpoint directory (by incrementing the checkpoint number) and prints out the following INFO message to the logs:</p> <pre><code>Created new checkpoint for stream [displayName] at [checkpoint_path].\n</code></pre> <p><code>reset</code> is used when:</p> <ul> <li><code>PipelineExecution</code> is requested to run a pipeline update (with full-refresh update)</li> </ul>"},{"location":"StreamListener/","title":"StreamListener","text":"<p><code>StreamListener</code> is a <code>StreamingQueryListener</code> (Spark Structured Streaming).</p>"},{"location":"StreamingFlow/","title":"StreamingFlow","text":"<p><code>StreamingFlow</code> is a ResolvedFlow that may or may not be append.</p> <p><code>StreamingFlow</code> represents an UnresolvedFlow with a streaming dataframe in a dataflow graph.</p> <p><code>StreamingFlow</code> is planned for execution as StreamingTableWrite (assuming that the Output of this flow's destination is a Table).</p>"},{"location":"StreamingFlow/#creating-instance","title":"Creating Instance","text":"<p><code>StreamingFlow</code> takes the following to be created:</p> <ul> <li> UnresolvedFlow <li> FlowFunctionResult <li>mustBeAppend flag</li> <p><code>StreamingFlow</code> is created when:</p> <ul> <li><code>FlowResolver</code> is requested to convertResolvedToTypedFlow (for an UnresolvedFlow with a streaming dataframe)</li> </ul>"},{"location":"StreamingFlow/#mustBeAppend","title":"mustBeAppend Flag","text":"<pre><code>mustBeAppend: Boolean\n</code></pre> <p><code>StreamingFlow</code> is given <code>mustBeAppend</code> flag when created.</p> <p>Default: <code>false</code></p> <p>The value of <code>mustBeAppend</code> flag is based on whether there are more than one flow to this flow's destination in a dataflow graph or not.</p> <p>When enabled (<code>true</code>), this UnresolvedFlow is planned for execution with the <code>Append</code> output mode (as the other flows will then get their results overwritten).</p> <p>StreamingTableWrite ignores mustBeAppend</p> <p><code>StreamingFlow</code> is planned for execution as StreamingTableWrite with no execution differences based on the <code>mustBeAppend</code> flag.</p>"},{"location":"StreamingFlowExecution/","title":"StreamingFlowExecution","text":"<p><code>StreamingFlowExecution</code> is an extension of the FlowExecution abstraction for streaming flow executions that process data statefully using Spark Structured Streaming.</p>"},{"location":"StreamingFlowExecution/#contract-subset","title":"Contract (Subset)","text":""},{"location":"StreamingFlowExecution/#checkpointPath","title":"Checkpoint Location","text":"<pre><code>checkpointPath: String\n</code></pre> <p>Used when:</p> <ul> <li><code>StreamingTableWrite</code> is requested to start a streaming query</li> </ul>"},{"location":"StreamingFlowExecution/#startStream","title":"Start Streaming Query","text":"<pre><code>startStream(): StreamingQuery\n</code></pre> <p>See:</p> <ul> <li>SinkWrite</li> <li>StreamingTableWrite</li> </ul> <p>Used when:</p> <ul> <li><code>StreamingFlowExecution</code> is requested to executeInternal</li> </ul>"},{"location":"StreamingFlowExecution/#trigger","title":"Streaming Trigger","text":"<pre><code>trigger: Trigger\n</code></pre> <p><code>Trigger</code> (Structured Streaming)</p> <p>See:</p> <ul> <li>SinkWrite</li> <li>StreamingTableWrite</li> </ul> <p>Used when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a StreamingFlow</li> <li><code>StreamingTableWrite</code> is requested to execute the streaming query</li> </ul>"},{"location":"StreamingFlowExecution/#implementations","title":"Implementations","text":"<ul> <li>SinkWrite</li> <li>StreamingTableWrite</li> </ul>"},{"location":"StreamingFlowExecution/#executeInternal","title":"executeInternal","text":"FlowExecution <pre><code>executeInternal(): Future[Unit]\n</code></pre> <p><code>executeInternal</code> is part of the FlowExecution abstraction.</p> <p><code>executeInternal</code> prints out the following INFO message to the logs:</p> <pre><code>Starting [identifier] with checkpoint location [checkpointPath]\n</code></pre> <p><code>executeInternal</code> starts the stream (with this SparkSession and sqlConf).</p> <p>In the end, <code>executeInternal</code> awaits termination of the <code>StreamingQuery</code>.</p> Final Method <p><code>executeInternal</code> is a Scala final method and may not be overridden in subclasses.</p> <p>Learn more in the Scala Language Specification.</p>"},{"location":"StreamingTableWrite/","title":"StreamingTableWrite Flow Execution","text":"<p><code>StreamingTableWrite</code> is a StreamingFlowExecution that writes a streaming <code>DataFrame</code> to a Table..</p> <p>When executed, <code>StreamingTableWrite</code> starts a streaming query to append new rows to an output table.</p>"},{"location":"StreamingTableWrite/#creating-instance","title":"Creating Instance","text":"<p><code>StreamingTableWrite</code> takes the following to be created:</p> <ul> <li> TableIdentifier <li> ResolvedFlow <li> DataflowGraph <li> PipelineUpdateContext <li> Checkpoint Location <li> Streaming Trigger <li> Destination (Table) <li> SQL Configuration <p><code>StreamingTableWrite</code> is created when:</p> <ul> <li><code>FlowPlanner</code> is requested to plan a StreamingFlow</li> </ul>"},{"location":"StreamingTableWrite/#startStream","title":"Execute Streaming Query","text":"StreamingFlowExecution <pre><code>startStream(): StreamingQuery\n</code></pre> <p><code>startStream</code> is part of the StreamingFlowExecution abstraction.</p> <p><code>startStream</code> builds the logical query plan of this flow's structured query (requesting the DataflowGraph to reanalyze this flow).</p> <p><code>startStream</code> creates a <code>DataStreamWriter</code> (Spark Structured Streaming) with the following:</p> <code>DataStreamWriter</code>'s Property Value <code>queryName</code> This displayName <code>checkpointLocation</code> option This checkpoint path <code>trigger</code> This streaming trigger <code>outputMode</code> Append (always) <code>format</code> The format of this output table (only when defined) <p>In the end, <code>startStream</code> starts the streaming write query to this output table.</p>"},{"location":"SystemMetadata/","title":"SystemMetadata","text":""},{"location":"SystemMetadata/#getLatestCheckpointDir","title":"getLatestCheckpointDir","text":"<pre><code>getLatestCheckpointDir(\n  rootDir: Path,\n  createNewCheckpointDir: Boolean = false): String\n</code></pre> <p><code>getLatestCheckpointDir</code>...FIXME</p> <p><code>getLatestCheckpointDir</code> is used when:</p> <ul> <li><code>FlowSystemMetadata</code> is requested to get the latest checkpoint location and get the latest checkpoint location if available</li> </ul>"},{"location":"Table/","title":"Table","text":"<p><code>Table</code> is a TableInput and an Output.</p>"},{"location":"Table/#load","title":"Load Data","text":"Input <pre><code>load(\n  readOptions: InputReadOptions): DataFrame\n</code></pre> <p><code>load</code> is part of the Input abstraction.</p> <p><code>load</code> is a \"shortcut\" to create a batch or a streaming <code>DataFrame</code> (based on the type of the given InputReadOptions).</p> <p>For StreamingReadOptions, <code>load</code> creates a <code>DataStreamReader</code> (Spark Structured Streaming) to load a table (using <code>DataStreamReader.table</code> operator) with the given <code>StreamingReadOptions</code>.</p> <p>For BatchReadOptions, <code>load</code> creates a DataFrameReader to load a table (using <code>DataFrameReader.table</code> operator).</p>"},{"location":"TableInput/","title":"TableInput","text":"<p><code>TableInput</code> is...FIXME</p>"},{"location":"TemporaryView/","title":"TemporaryView","text":"<p><code>TemporaryView</code> is...FIXME</p>"},{"location":"TriggeredGraphExecution/","title":"TriggeredGraphExecution","text":"<p><code>TriggeredGraphExecution</code> is a GraphExecution for the DataflowGraph and PipelineUpdateContext.</p>"},{"location":"TriggeredGraphExecution/#creating-instance","title":"Creating Instance","text":"<p><code>TriggeredGraphExecution</code> takes the following to be created:</p> <ul> <li> DataflowGraph <li> PipelineUpdateContext <li> <code>onCompletion</code> Callback (<code>RunTerminationReason =&gt; Unit</code>, default: do nothing) <li> <code>Clock</code> (default: <code>SystemClock</code>) <p><code>TriggeredGraphExecution</code> is created when:</p> <ul> <li><code>PipelineExecution</code> is requested to run a pipeline update</li> </ul>"},{"location":"TriggeredGraphExecution/#topologicalExecutionThread","title":"Topological Execution Thread","text":"<pre><code>topologicalExecutionThread: Option[Thread]\n</code></pre> <p><code>topologicalExecutionThread</code> is the Topological Execution thread of execution of this pipeline update.</p> <p><code>topologicalExecutionThread</code> is initialized and started when <code>TriggeredGraphExecution</code> is requested to start.</p> <p><code>topologicalExecutionThread</code> runs until awaitCompletion or stopInternal.</p>"},{"location":"TriggeredGraphExecution/#start","title":"Start Pipeline","text":"GraphExecution <pre><code>start(): Unit\n</code></pre> <p><code>start</code> is part of the GraphExecution abstraction.</p> <p><code>start</code> registers the stream listener.</p> <p><code>start</code> requests this PipelineUpdateContext for the flows to be refreshed and...FIXME</p> <p><code>start</code> creates a Topological Execution thread and starts its execution.</p>"},{"location":"TriggeredGraphExecution/#buildTopologicalExecutionThread","title":"Create Topological Execution Thread","text":"<pre><code>buildTopologicalExecutionThread(): Thread\n</code></pre> <p><code>buildTopologicalExecutionThread</code> creates a new thread of execution known as Topological Execution.</p> <p>When started, the thread does topological execution.</p>"},{"location":"TriggeredGraphExecution/#topologicalExecution","title":"topologicalExecution","text":"<pre><code>topologicalExecution(): Unit\n</code></pre> <p><code>topologicalExecution</code> finds the flows in <code>QUEUED</code> and <code>RUNNING</code> states or failed but can be re-tried.</p> <p>For each flow in <code>RUNNING</code> state, <code>topologicalExecution</code>...FIXME</p> <p><code>topologicalExecution</code> checks leaking permits.</p> <p>FIXME Explain</p> <p><code>topologicalExecution</code> starts flows that are ready to start.</p>"},{"location":"TriggeredGraphExecution/#startFlow","title":"Start Single Flow","text":"<pre><code>startFlow(\n  flow: ResolvedFlow): Unit\n</code></pre> <p><code>startFlow</code> prints out the following INFO message to the logs:</p> <pre><code>Starting flow [flow_identifier]\n</code></pre> <p><code>startFlow</code> requests this PipelineUpdateContext for the FlowProgressEventLogger to recordPlanningForBatchFlow.</p> <p><code>startFlow</code> planAndStartFlow.</p> <p><code>startFlow</code>...FIXME</p>"},{"location":"TriggeredGraphExecution/#streamTrigger","title":"Streaming Trigger","text":"GraphExecution <pre><code>streamTrigger(\n  flow: Flow): Trigger\n</code></pre> <p><code>streamTrigger</code> is part of the GraphExecution abstraction.</p> <p><code>streamTrigger</code> is <code>AvailableNowTrigger</code> (Spark Structured Streaming).</p>"},{"location":"TriggeredGraphExecution/#awaitCompletion","title":"awaitCompletion","text":"GraphExecution <pre><code>awaitCompletion(): Unit\n</code></pre> <p><code>awaitCompletion</code> is part of the GraphExecution abstraction.</p> <p><code>awaitCompletion</code> waits for this Topological Execution thread to die.</p>"},{"location":"TriggeredGraphExecution/#stopInternal","title":"stopInternal","text":"<pre><code>stopInternal(\n  stopTopologicalExecutionThread: Boolean): Unit\n</code></pre> <p><code>stopInternal</code>...FIXME</p> <p><code>stopInternal</code> is used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to start and stop</li> </ul>"},{"location":"TriggeredGraphExecution/#stop","title":"Stop Execution","text":"GraphExecution <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code> is part of the GraphExecution abstraction.</p> <p><code>stop</code> stopInternal (with <code>stopTopologicalExecutionThread</code> flag enabled).</p>"},{"location":"UnresolvedFlow/","title":"UnresolvedFlow","text":"<p><code>UnresolvedFlow</code> is a Flow that represents a flow in the Python and SQL transformations in Spark Declarative Pipelines:</p> <ul> <li>register_flow in PySpark's decorators</li> <li>CREATE FLOW AS INSERT INTO BY NAME</li> <li>CREATE MATERIALIZED VIEW</li> <li>CREATE STREAMING TABLE AS</li> <li>CREATE VIEW and the other variants of CREATE VIEW</li> </ul> <p><code>UnresolvedFlow</code> is registered to a GraphRegistrationContext with register a flow.</p> <p><code>UnresolvedFlow</code> is analyzed and resolved to ResolvedFlow (by FlowResolver when DataflowGraph is requested to resolve).</p> <p><code>UnresolvedFlow</code> must have unique identifiers (or an <code>AnalysisException</code> is reported).</p>"},{"location":"UnresolvedFlow/#creating-instance","title":"Creating Instance","text":"<p><code>UnresolvedFlow</code> takes the following to be created:</p> <ul> <li> <code>TableIdentifier</code> <li> Flow destination (<code>TableIdentifier</code>) <li> <code>FlowFunction</code> <li> <code>QueryContext</code> <li> SQL Config <li>once flag</li> <li> <code>QueryOrigin</code> <p><code>UnresolvedFlow</code> is created when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define a flow</li> <li><code>SqlGraphRegistrationContext</code> is requested to handle the following SQL queries:<ul> <li>CreateFlowCommand</li> <li>CreateMaterializedViewAsSelect</li> <li>CreateView</li> <li>CreateStreamingTableAsSelect</li> <li>CreateViewCommand</li> </ul> </li> </ul>"},{"location":"UnresolvedFlow/#once","title":"once Flag","text":"<p><code>UnresolvedFlow</code> is given the once flag when created.</p> <p><code>once</code> flag is disabled (<code>false</code>) explicitly for the following:</p> <ul> <li>CreateFlowHandler</li> <li>CreateMaterializedViewAsSelectHandler</li> <li>CreatePersistedViewCommandHandler</li> <li>CreateStreamingTableAsSelectHandler</li> <li>CreateTemporaryViewHandler</li> <li><code>PipelinesHandler</code> is requested to define a flow</li> </ul> <p>No ONCE UnresolvedFlows</p> <p>It turns out that all <code>UnresolvedFlow</code>s created are not ONCE flows.</p> <p>As per this commit, it is said that:</p> <p>However, the server does not currently implement this behavior yet. To avoid accidentally releasing APIs that don't actually work, we should take these arguments out for now. And add them back in when we actually support this functionality.</p>"},{"location":"View/","title":"View","text":"<p><code>View</code> is an extension of the GraphElement abstraction for dataflow graph elements that represent persisted and temporary views in a declarative pipeline.</p>"},{"location":"View/#contract","title":"Contract","text":""},{"location":"View/#comment","title":"Comment","text":"<pre><code>comment: Option[String]\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define an output</li> <li><code>SqlGraphRegistrationContext</code> is requested to process the SQL queries:<ul> <li><code>CREATE VIEW</code></li> <li><code>CREATE TEMPORARY VIEW</code></li> </ul> </li> </ul>"},{"location":"View/#properties","title":"Properties","text":"<pre><code>properties: Map[String, String]\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define an output</li> <li><code>SqlGraphRegistrationContext</code> is requested to process the SQL queries:<ul> <li><code>CREATE VIEW</code></li> <li><code>CREATE TEMPORARY VIEW</code></li> </ul> </li> <li><code>DatasetManager</code> is requested to materialize a view</li> </ul>"},{"location":"View/#sqlText","title":"SQL Text","text":"<pre><code>sqlText: Option[String]\n</code></pre> <p>Used when:</p> <ul> <li><code>PipelinesHandler</code> is requested to define an output</li> <li><code>SqlGraphRegistrationContext</code> is requested to process the SQL queries:<ul> <li><code>CREATE VIEW</code></li> <li><code>CREATE TEMPORARY VIEW</code></li> </ul> </li> </ul>"},{"location":"View/#implementations","title":"Implementations","text":"<ul> <li>PersistedView</li> <li>TemporaryView</li> </ul>"},{"location":"VirtualTableInput/","title":"VirtualTableInput","text":"<p><code>VirtualTableInput</code> is...FIXME</p>"},{"location":"configuration-properties/","title":"Configuration Properties","text":"<p>spark.sql.pipelines is a family of the configuration properties (Spark SQL) for Spark Declarative Pipelines.</p>"},{"location":"configuration-properties/#spark.sql.pipelines.execution.streamstate.pollingInterval","title":"execution.streamstate.pollingInterval","text":"<p>spark.sql.pipelines.execution.streamstate.pollingInterval</p> <p>(internal) How often (in seconds) the stream state is polled for changes. This is used to check if the stream has failed and needs to be restarted.</p> <p>Default: <code>1</code></p> <p>Use SQLConf.PIPELINES_STREAM_STATE_POLLING_INTERVAL to reference the name.</p> <p>Use SQLConf.streamStatePollingInterval method to access the current value.</p> <p>Used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to topologicalExecution</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.execution.watchdog.minRetryTime","title":"execution.watchdog.minRetryTime","text":"<p>spark.sql.pipelines.execution.watchdog.minRetryTime</p> <p>(internal) Initial duration (in seconds) between the time when we notice a flow has failed and when we try to restart the flow. The interval between flow restarts doubles with every stream failure up to the maximum value set in spark.sql.pipelines.execution.watchdog.maxRetryTime.</p> <p>Default: <code>5</code> (seconds)</p> <p>Must be at least 1 second</p> <p>Use SQLConf.PIPELINES_WATCHDOG_MIN_RETRY_TIME_IN_SECONDS to reference the name.</p> <p>Use SQLConf.watchdogMinRetryTimeInSeconds method to access the current value.</p> <p>Used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to backoffStrategy</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.execution.watchdog.maxRetryTime","title":"execution.watchdog.maxRetryTime","text":"<p>spark.sql.pipelines.execution.watchdog.maxRetryTime</p> <p>(internal) Maximum time interval (in seconds) at which flows will be restarted</p> <p>Default: <code>3600</code> (seconds)</p> <p>Must be greater than or equal to spark.sql.pipelines.execution.watchdog.minRetryTime</p> <p>Use SQLConf.PIPELINES_WATCHDOG_MAX_RETRY_TIME_IN_SECONDS to reference the name.</p> <p>Use SQLConf.watchdogMaxRetryTimeInSeconds method to access the current value.</p> <p>Used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested to backoffStrategy</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.execution.maxConcurrentFlows","title":"execution.maxConcurrentFlows","text":"<p>spark.sql.pipelines.execution.maxConcurrentFlows</p> <p>(internal) Maximum number of flows to execute at once. Used to tune performance for triggered pipelines. Has no effect on continuous pipelines.</p> <p>Default: <code>16</code></p> <p>Use SQLConf.PIPELINES_MAX_CONCURRENT_FLOWS to reference the name.</p> <p>Use SQLConf.maxConcurrentFlows method to access the current value.</p> <p>Used when:</p> <ul> <li><code>TriggeredGraphExecution</code> is requested for the concurrencyLimit and to topologicalExecution</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.timeoutMsForTerminationJoinAndLock","title":"timeoutMsForTerminationJoinAndLock","text":"<p>spark.sql.pipelines.timeoutMsForTerminationJoinAndLock</p> <p>(internal) Timeout (in ms) to grab a lock for stopping update - default is 1hr.</p> <p>Default: <code>60 * 60 * 1000</code> (1 hour)</p> <p>Must be at least 1 millisecond</p> <p>Use SQLConf.PIPELINES_TIMEOUT_MS_FOR_TERMINATION_JOIN_AND_LOCK to reference the name.</p> <p>Use SQLConf.timeoutMsForTerminationJoinAndLock method to access the current value.</p> <p>Used when:</p> <ul> <li><code>GraphExecution</code> is requested to stopThread</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.maxFlowRetryAttempts","title":"maxFlowRetryAttempts","text":"<p>spark.sql.pipelines.maxFlowRetryAttempts</p> <p>Maximum number of times a flow can be retried. Can be set at the pipeline or flow level</p> <p>Default: <code>2</code></p> <p>Use SQLConf.PIPELINES_MAX_FLOW_RETRY_ATTEMPTS to reference the name.</p> <p>Use SQLConf.maxFlowRetryAttempts method to access the current value.</p> <p>Used when:</p> <ul> <li><code>GraphExecution</code> is requested to maxRetryAttemptsForFlow</li> </ul>"},{"location":"configuration-properties/#spark.sql.pipelines.event.queue.capacity","title":"event.queue.capacity","text":"<p>spark.sql.pipelines.event.queue.capacity</p> <p>(internal) Capacity of the event queue used in pipelined execution. When the queue is full, non-terminal <code>FlowProgressEvent</code>s will be dropped.</p> <p>Default: <code>1000</code></p> <p>Must be positive</p> <p>Use SQLConf.PIPELINES_EVENT_QUEUE_CAPACITY to reference the name.</p> <p>Used when:</p> <ul> <li><code>PipelineEventSender</code> is created</li> </ul>"},{"location":"overview/","title":"Spark Declarative Pipelines","text":"<p>Spark Declarative Pipelines (SDP) is a declarative framework for building data processing (ETL) pipelines on Apache Spark in Python and SQL languages.</p> <p>A Declarative Pipelines project is defined and configured in a pipeline specification file.</p> <p>A Declarative Pipelines project can be executed with spark-pipelines shell script.</p> <p>Declarative Pipelines uses Python decorators to describe tables, views and flows, declaratively.</p> <p>The definitions of tables, views and flows are registered in DataflowGraphRegistry (with GraphRegistrationContexts by graph IDs). A <code>GraphRegistrationContext</code> is converted into a DataflowGraph when <code>PipelinesHandler</code> is requested to start a pipeline run (when spark-pipelines script is launched with <code>run</code> or <code>dry-run</code> command).</p> <p>Streaming flows are backed by streaming sources, and batch flows are backed by batch sources.</p> <p>DataflowGraph is the core graph structure in Declarative Pipelines.</p> <p>Once described, a pipeline can be started (on a PipelineExecution).</p>"},{"location":"overview/#spark.sql.pipelines","title":"Configuration Properties","text":"<p>spark.sql.pipelines Configuration Properties</p>"},{"location":"overview/#pipeline-specification-file","title":"Pipeline Specification File","text":"<p>A Declarative Pipelines project is defined using a pipeline specification file (in YAML format).</p> <p>Unless specified using spark-pipelines CLI's --spec option, Declarative Pipelines uses the following file names as the defaults:</p> <ul> <li><code>spark-pipeline.yml</code></li> <li><code>spark-pipeline.yaml</code></li> </ul> <p>In the pipeline specification file, Declarative Pipelines developers specify files (<code>libraries</code>) with tables, views and flows (transformations) definitions in Python and SQL. A SDP project can use both languages simultaneously.</p> <p>The following fields are supported:</p> Field Name Description <code>name</code> (required) <code>storage</code> (required) The root storage location of pipeline metadata (e.g., checkpoints for streaming flows).SPARK-53751 Explicit Checkpoint Location <code>catalog</code> The default catalog to register datasets into.Unless specified, PipelinesHandler falls back to the current catalog. <code>database</code> The default database to register datasets intoUnless specified, PipelinesHandler falls back to the current database. <code>schema</code> Alias of <code>database</code>. Used unless <code>database</code> is defined <code>configuration</code> SparkSession configsSpark Pipelines runtime uses the configs to build a new <code>SparkSession</code> when <code>run</code>.spark.sql.connect.serverStacktrace.enabled is hardcoded to be always <code>false</code>. <code>libraries</code> <code>glob</code>s of <code>include</code>s with transformations in SQL and Python Info <p>Pipeline spec is resolved in <code>pyspark/pipelines/cli.py::unpack_pipeline_spec</code>.</p> <pre><code>name: hello-spark-pipelines\ncatalog: default_catalog\nschema: default\nstorage: file:///absolute/path/to/storage/dir\nconfiguration:\n  spark.key1: value1\nlibraries:\n  - glob:\n      include: transformations/**\n</code></pre>"},{"location":"overview/#spark-pipelines","title":"Spark Pipelines CLI","text":"<p><code>spark-pipelines</code> shell script is the Spark Pipelines CLI (that launches org.apache.spark.deploy.SparkPipelines behind the scenes).</p>"},{"location":"overview/#dataset-types","title":"Dataset Types","text":"<p>Declarative Pipelines supports the following dataset types:</p> <ul> <li>Append Flows</li> <li>Materialized Views</li> <li>Streaming tables</li> <li>Table that are published to a catalog.</li> <li>Views that are not published to a catalog.</li> </ul>"},{"location":"overview/#append-flows","title":"Append Flows","text":"<p>Append Flows can be created with the following:</p> <ul> <li>@dp.append_flow</li> <li>CREATE FLOW AS INSERT INTO BY NAME</li> </ul>"},{"location":"overview/#materialized-views","title":"Materialized Views","text":"<p>Materialized Views can be created with the following:</p> <ul> <li>@dp.materialized_view</li> <li>CREATE MATERIALIZED VIEW AS</li> </ul> <p>Materialized Views are published to a catalog.</p>"},{"location":"overview/#streaming-tables","title":"Streaming Tables","text":"<p>Streaming tables are tables whose content is produced by one or more streaming flows.</p> <p>Streaming tables can be created with the following:</p> <ul> <li>@dp.create_streaming_table or CREATE STREAMING TABLE (with no flows that can be defined later with @dp.append_flow or CREATE FLOW AS INSERT INTO BY NAME)</li> <li>CREATE STREAMING TABLE AS</li> </ul>"},{"location":"overview/#spark-connect","title":"Spark Connect Only","text":"<p>Declarative Pipelines currently only supports Spark Connect.</p> <pre><code>$ ./bin/spark-pipelines --conf spark.api.mode=xxx\n...\n25/08/03 12:33:57 INFO SparkPipelines: --spark.api.mode must be 'connect'. Declarative Pipelines currently only supports Spark Connect.\nException in thread \"main\" org.apache.spark.SparkUserAppException: User application exited with 1\n at org.apache.spark.deploy.SparkPipelines$$anon$1.handle(SparkPipelines.scala:73)\n at org.apache.spark.launcher.SparkSubmitOptionParser.parse(SparkSubmitOptionParser.java:169)\n at org.apache.spark.deploy.SparkPipelines$$anon$1.&lt;init&gt;(SparkPipelines.scala:58)\n at org.apache.spark.deploy.SparkPipelines$.splitArgs(SparkPipelines.scala:57)\n at org.apache.spark.deploy.SparkPipelines$.constructSparkSubmitArgs(SparkPipelines.scala:43)\n at org.apache.spark.deploy.SparkPipelines$.main(SparkPipelines.scala:37)\n at org.apache.spark.deploy.SparkPipelines.main(SparkPipelines.scala)\n</code></pre>"},{"location":"overview/#python","title":"Python","text":"<p>Python API</p>"},{"location":"overview/#sql","title":"SQL","text":"<p>SQL</p>"},{"location":"overview/#demo-create-virtual-environment-for-python-client","title":"Demo: Create Virtual Environment for Python Client","text":"<pre><code>uv init hello-spark-pipelines &amp;&amp; cd hello-spark-pipelines\n</code></pre> <pre><code>export SPARK_HOME=/Users/jacek/oss/spark\n</code></pre> <pre><code>uv add --editable $SPARK_HOME/python/packaging/client\n</code></pre> <pre><code>uv tree --depth 2\n</code></pre> Output <pre><code>hello-spark-pipelines v0.1.0\n\u2514\u2500\u2500 pyspark-client v4.2.0.dev0\n    \u251c\u2500\u2500 googleapis-common-protos v1.72.0\n    \u251c\u2500\u2500 grpcio v1.76.0\n    \u251c\u2500\u2500 grpcio-status v1.76.0\n    \u251c\u2500\u2500 numpy v2.3.4\n    \u251c\u2500\u2500 pandas v2.3.3\n    \u251c\u2500\u2500 pyarrow v22.0.0\n    \u251c\u2500\u2500 pyyaml v6.0.3\n    \u2514\u2500\u2500 zstandard v0.25.0\n</code></pre> <pre><code>uv pip list\n</code></pre> Output <pre><code>Package                  Version     Editable project location\n------------------------ ----------- ----------------------------------------------\ngoogleapis-common-protos 1.72.0\ngrpcio                   1.76.0\ngrpcio-status            1.76.0\nnumpy                    2.3.4\npandas                   2.3.3\nprotobuf                 6.33.1\npyarrow                  22.0.0\npyspark-client           4.2.0.dev0  /Users/jacek/oss/spark/python/packaging/client\npython-dateutil          2.9.0.post0\npytz                     2025.2\npyyaml                   6.0.3\nsix                      1.17.0\ntyping-extensions        4.15.0\ntzdata                   2025.2\nzstandard                0.25.0\n</code></pre> <p>Activate (source) the virtual environment (that <code>uv</code> helped us create).</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>This activation brings all the necessary Spark Declarative Pipelines Python dependencies (that are only available in the source format only) for non-<code>uv</code> tools and CLI, incl. Spark Pipelines CLI itself.</p> <pre><code>$SPARK_HOME/bin/spark-pipelines --help\n</code></pre> <pre><code>usage: cli.py [-h] {run,dry-run,init} ...\n\nPipelines CLI\n\npositional arguments:\n  {run,dry-run,init}\n    run               Run a pipeline. If no refresh options specified, a\n                      default incremental update is performed.\n    dry-run           Launch a run that just validates the graph and checks\n                      for errors.\n    init              Generate a sample pipeline project, with a spec file and\n                      example transformations.\n\noptions:\n  -h, --help          show this help message and exit\n</code></pre> <p>macOS and PYSPARK_PYTHON</p> <p>On macOS, you may want to define <code>PYSPARK_PYTHON</code> environment variable to point at Python &gt;= 3.10.</p> <pre><code>export PYSPARK_PYTHON=python3.14\n</code></pre>"},{"location":"overview/#demo-python-api","title":"Demo: Python API","text":"Activate Virtual Environment <p>Follow Demo: Create Virtual Environment for Python Client before getting started with this demo.</p> <p>In a terminal, start a Spark Connect Server.</p> <pre><code>./sbin/start-connect-server.sh\n</code></pre> <p>It will listen on port 15002.</p> Monitor Logs <pre><code>tail -f logs/*org.apache.spark.sql.connect.service.SparkConnectServer*.out\n</code></pre> <p>Start a Spark Connect-enabled PySpark shell.</p> <pre><code>$SPARK_HOME/bin/pyspark --remote sc://localhost:15002\n</code></pre> <pre><code>from pyspark.pipelines.spark_connect_pipeline import create_dataflow_graph\ndataflow_graph_id = create_dataflow_graph(\n  spark,\n  default_catalog=None,\n  default_database=None,\n  sql_conf=None,\n)\n\n# &gt;&gt;&gt; print(dataflow_graph_id)\n# 3cb66d5a-0621-4f15-9920-e99020e30e48\n</code></pre> <pre><code>from pyspark.pipelines.spark_connect_graph_element_registry import SparkConnectGraphElementRegistry\nregistry = SparkConnectGraphElementRegistry(spark, dataflow_graph_id)\n</code></pre> <pre><code>from pyspark import pipelines as dp\n</code></pre> <pre><code>from pyspark.pipelines.graph_element_registry import graph_element_registration_context\nwith graph_element_registration_context(registry):\n  dp.create_streaming_table(\"demo_streaming_table\")\n</code></pre> <p>You should see the following INFO message in the logs of the Spark Connect Server:</p> <pre><code>INFO PipelinesHandler: Define pipelines dataset cmd received: define_dataset {\n  dataflow_graph_id: \"3cb66d5a-0621-4f15-9920-e99020e30e48\"\n  dataset_name: \"demo_streaming_table\"\n  dataset_type: TABLE\n}\n</code></pre>"},{"location":"overview/#demo-spark-pipelines-cli","title":"Demo: spark-pipelines CLI","text":"<p>Activate Virtual Environment</p> <p>Follow Demo: Create Virtual Environment for Python Client before getting started with this demo.</p>"},{"location":"overview/#1-display-pipelines-help","title":"1\ufe0f\u20e3 Display Pipelines Help","text":"<p>Run <code>spark-pipelines --help</code> to learn the options.</p> <pre><code>$SPARK_HOME/bin/spark-pipelines --help\n</code></pre> Output <pre><code>usage: cli.py [-h] {run,dry-run,init} ...\n\nPipelines CLI\n\npositional arguments:\n  {run,dry-run,init}\n    run               Run a pipeline. If no refresh options specified, a\n                      default incremental update is performed.\n    dry-run           Launch a run that just validates the graph and checks\n                      for errors.\n    init              Generate a sample pipeline project, including a spec\n                      file and example transformations.\n\noptions:\n  -h, --help          show this help message and exit\n</code></pre>"},{"location":"overview/#2-create-pipelines-demo-project","title":"2\ufe0f\u20e3 Create Pipelines Demo Project","text":"<p>You've only created an empty Python project so far (using <code>uv</code>).</p> <p>Create a demo double <code>hello-spark-pipelines</code> pipelines project with a sample <code>spark-pipeline.yml</code> and sample transformations (in Python and in SQL).</p> <pre><code>$SPARK_HOME/bin/spark-pipelines init --name hello-spark-pipelines &amp;&amp; \\\nmv hello-spark-pipelines/* . &amp;&amp; \\\nrm -rf hello-spark-pipelines\n</code></pre> <pre><code>cat spark-pipeline.yml\n</code></pre> Output <pre><code>name: hello-spark-pipelines\nstorage: file:///Users/jacek/sandbox/hello-spark-pipelines/hello-spark-pipelines/pipeline-storage\nlibraries:\n  - glob:\n      include: transformations/**\n</code></pre> <pre><code>tree transformations\n</code></pre> Output <pre><code>transformations\n\u251c\u2500\u2500 example_python_materialized_view.py\n\u2514\u2500\u2500 example_sql_materialized_view.sql\n\n1 directory, 2 files\n</code></pre> <p>Spark Connect Server should be down</p> <p><code>spark-pipelines dry-run</code> starts its own Spark Connect Server at 15002 port (unless started with <code>--remote</code> option).</p> <p>Shut down Spark Connect Server if you started it already.</p> <pre><code>$SPARK_HOME/sbin/stop-connect-server.sh\n</code></pre> <p><code>--remote</code> option</p> <p>Use <code>--remote</code> option to connect to a standalone Spark Connect Server.</p> <pre><code>$SPARK_HOME/bin/spark-pipelines --remote sc://localhost dry-run\n</code></pre>"},{"location":"overview/#3-dry-run-pipelines-project","title":"3\ufe0f\u20e3 Dry Run Pipelines Project","text":"<pre><code>$SPARK_HOME/bin/spark-pipelines dry-run\n</code></pre> Output <pre><code>Loading pipeline spec from /Users/jacek/sandbox/hello-spark-pipelines/spark-pipeline.yml...\nCreating Spark session...\nCreating dataflow graph...\nRegistering graph elements...\nLoading definitions. Root directory: '/Users/jacek/sandbox/hello-spark-pipelines'.\nFound 2 files matching glob 'transformations/**/*'\nImporting /Users/jacek/sandbox/hello-spark-pipelines/transformations/example_python_materialized_view.py...\nRegistering SQL file /Users/jacek/sandbox/hello-spark-pipelines/transformations/example_sql_materialized_view.sql...\nStarting run...\nRun is COMPLETED.\n</code></pre>"},{"location":"overview/#4-run-pipelines-project","title":"4\ufe0f\u20e3 Run Pipelines Project","text":"<p>Run the pipeline.</p> <pre><code>$SPARK_HOME/bin/spark-pipelines run\n</code></pre> Output <pre><code>Loading pipeline spec from /Users/jacek/sandbox/hello-spark-pipelines/spark-pipeline.yml...\nCreating Spark session...\nCreating dataflow graph...\nRegistering graph elements...\nLoading definitions. Root directory: '/Users/jacek/sandbox/hello-spark-pipelines'.\nFound 2 files matching glob 'transformations/**/*'\nImporting /Users/jacek/sandbox/hello-spark-pipelines/transformations/example_python_materialized_view.py...\nRegistering SQL file /Users/jacek/sandbox/hello-spark-pipelines/transformations/example_sql_materialized_view.sql...\nStarting run...\nFlow spark_catalog.default.example_python_materialized_view is QUEUED.\nFlow spark_catalog.default.example_sql_materialized_view is QUEUED.\nFlow spark_catalog.default.example_python_materialized_view is PLANNING.\nFlow spark_catalog.default.example_python_materialized_view is STARTING.\nFlow spark_catalog.default.example_python_materialized_view is RUNNING.\nFlow spark_catalog.default.example_python_materialized_view has COMPLETED.\nFlow spark_catalog.default.example_sql_materialized_view is PLANNING.\nFlow spark_catalog.default.example_sql_materialized_view is STARTING.\nFlow spark_catalog.default.example_sql_materialized_view is RUNNING.\nFlow spark_catalog.default.example_sql_materialized_view has COMPLETED.\nRun is COMPLETED.\n</code></pre> <pre><code>tree spark-warehouse\n</code></pre> Output <pre><code>spark-warehouse\n\u251c\u2500\u2500 example_python_materialized_view\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 _SUCCESS\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-00000-284bc03a-3405-4e8e-bbd7-f6f17d79c282-c000.snappy.parquet\n\u2514\u2500\u2500 example_sql_materialized_view\n    \u251c\u2500\u2500 _SUCCESS\n    \u2514\u2500\u2500 part-00000-8316b6c6-7532-4f7a-92f6-2ec024e069f4-c000.snappy.parquet\n\n3 directories, 4 files\n</code></pre>"},{"location":"overview/#demo-scala-api","title":"Demo: Scala API","text":""},{"location":"overview/#step-1-register-dataflow-graph","title":"Step 1. Register Dataflow Graph","text":"<p>DataflowGraphRegistry</p> <pre><code>import org.apache.spark.sql.connect.pipelines.DataflowGraphRegistry\n\nval graphId = DataflowGraphRegistry.createDataflowGraph(\n  defaultCatalog=spark.catalog.currentCatalog(),\n  defaultDatabase=spark.catalog.currentDatabase,\n  defaultSqlConf=Map.empty)\n</code></pre>"},{"location":"overview/#step-2-look-up-dataflow-graph","title":"Step 2. Look Up Dataflow Graph","text":"<p>DataflowGraphRegistry</p> <pre><code>import org.apache.spark.sql.pipelines.graph.GraphRegistrationContext\n\nval graphCtx: GraphRegistrationContext =\n  DataflowGraphRegistry.getDataflowGraphOrThrow(dataflowGraphId=graphId)\n</code></pre>"},{"location":"overview/#step-3-create-dataflowgraph","title":"Step 3. Create DataflowGraph","text":"<p>GraphRegistrationContext</p> <pre><code>import org.apache.spark.sql.pipelines.graph.DataflowGraph\n\nval dp: DataflowGraph = graphCtx.toDataflowGraph\n</code></pre>"},{"location":"overview/#step-4-create-update-context","title":"Step 4. Create Update Context","text":"<p>PipelineUpdateContextImpl</p> <pre><code>import org.apache.spark.sql.pipelines.graph.{ PipelineUpdateContext, PipelineUpdateContextImpl }\nimport org.apache.spark.sql.pipelines.logging.PipelineEvent\n\nval swallowEventsCallback: PipelineEvent =&gt; Unit = _ =&gt; ()\n\nval updateCtx: PipelineUpdateContext =\n  new PipelineUpdateContextImpl(unresolvedGraph=dp, eventCallback=swallowEventsCallback)\n</code></pre>"},{"location":"overview/#step-5-start-pipeline","title":"Step 5. Start Pipeline","text":"<p>PipelineExecution</p> <pre><code>updateCtx.pipelineExecution.runPipeline()\n</code></pre>"},{"location":"overview/#learning-resources","title":"Learning Resources","text":"<ul> <li>Spark Declarative Pipelines Programming Guide</li> </ul>"},{"location":"python/","title":"Python","text":""},{"location":"python/#python-import-alias-convention","title":"Python Import Alias Convention","text":"<p>As of this commit 6ab0df9, the convention to alias the import of Declarative Pipelines in Python is <code>dp</code>.</p> <pre><code>from pyspark import pipelines as dp\n</code></pre>"},{"location":"python/#pyspark_pipelines","title":"pyspark.pipelines Python Module","text":"<p><code>pyspark.pipelines</code> module (in <code>__init__.py</code>) imports <code>pyspark.pipelines.api</code> module to expose the following Python functions (incl. decorators) to wildcard imports:</p> <ul> <li>append_flow</li> <li>create_sink</li> <li>create_streaming_table</li> <li>materialized_view</li> <li>table</li> <li>temporary_view</li> </ul> <p>Use the following import in your Python code:</p> <pre><code>from pyspark import pipelines as dp\n</code></pre>"},{"location":"python/#python-decorators","title":"Python Decorators","text":"<p>Declarative Pipelines uses Python decorators to define tables and views.</p> Decorator Purpose @dp.append_flow Append-only flows @dp.materialized_view Materialized views (with supporting flows) @dp.table Streaming and batch tables (with supporting flows) @dp.temporary_view Temporary views (with supporting flows)"},{"location":"python/#append_flow","title":"@dp.append_flow","text":"<pre><code>append_flow(\n    *,\n    target: str,\n    name: Optional[str] = None,\n    spark_conf: Optional[Dict[str, str]] = None,\n) -&gt; Callable[[QueryFunction], None] # (1)!\n</code></pre> <ol> <li><code>QueryFunction = Callable[[], DataFrame]</code> is a Python function that takes no arguments and returns a PySpark <code>DataFrame</code>.</li> </ol> <p>Registers an append Flow (in the active GraphElementRegistry)</p> <p><code>target</code> is the name of the dataset (destination) this flow writes to.</p>"},{"location":"python/#create_sink","title":"dp.create_sink","text":"<pre><code>create_sink(\n    name: str,\n    format: str,\n    options: Optional[Dict[str, str]] = None,\n) -&gt; None\n</code></pre> <p>Registers a Sink output in the active GraphElementRegistry.</p> Not Python Decorator <p>Unlike the others, <code>create_sink</code> is not a Python decorator (<code>Callable</code>).</p>"},{"location":"python/#create_streaming_table","title":"dp.create_streaming_table","text":"<pre><code>create_streaming_table(\n    name: str,\n    *,\n    comment: Optional[str] = None,\n    table_properties: Optional[Dict[str, str]] = None,\n    partition_cols: Optional[List[str]] = None,\n    cluster_by: Optional[List[str]] = None,\n    schema: Optional[Union[StructType, str]] = None,\n    format: Optional[str] = None,\n) -&gt; None\n</code></pre> Not Python Decorator <p>Unlike the others, <code>create_streaming_table</code> is not a Python decorator (<code>Callable</code>).</p> <p>Registers a <code>StreamingTable</code> dataset (in the active GraphElementRegistry) for Append Flows.</p>"},{"location":"python/#materialized_view","title":"@dp.materialized_view","text":"<pre><code>materialized_view(\n    query_function: Optional[QueryFunction] = None,\n    *,\n    name: Optional[str] = None,\n    comment: Optional[str] = None,\n    spark_conf: Optional[Dict[str, str]] = None,\n    table_properties: Optional[Dict[str, str]] = None,\n    partition_cols: Optional[List[str]] = None,\n    cluster_by: Optional[List[str]] = None,\n    schema: Optional[Union[StructType, str]] = None,\n    format: Optional[str] = None,\n) -&gt; Union[Callable[[QueryFunction], None], None]\n</code></pre> <p>Registers a MaterializedView dataset with an accompanying Flow in the active GraphElementRegistry.</p>"},{"location":"python/#table","title":"@dp.table","text":"<pre><code>table(\n    query_function: Optional[QueryFunction] = None,\n    *,\n    name: Optional[str] = None,\n    comment: Optional[str] = None,\n    spark_conf: Optional[Dict[str, str]] = None,\n    table_properties: Optional[Dict[str, str]] = None,\n    partition_cols: Optional[List[str]] = None,\n    cluster_by: Optional[List[str]] = None,\n    schema: Optional[Union[StructType, str]] = None,\n    format: Optional[str] = None,\n) -&gt; Union[Callable[[QueryFunction], None], None]\n</code></pre> <p>Registers a <code>StreamingTable</code> dataset with an accompanying Flow in the active GraphElementRegistry.</p>"},{"location":"python/#temporary_view","title":"@dp.temporary_view","text":"<pre><code>temporary_view(\n    query_function: Optional[QueryFunction] = None,\n    *,\n    name: Optional[str] = None,\n    comment: Optional[str] = None,\n    spark_conf: Optional[Dict[str, str]] = None,\n) -&gt; Union[Callable[[QueryFunction], None], None]\n</code></pre> <p>Registers a <code>TemporaryView</code> dataset with an accompanying Flow in the active GraphElementRegistry.</p>"},{"location":"spark_connect_pipeline/","title":"spark_connect_pipeline PySpark Module","text":""},{"location":"spark_connect_pipeline/#create_dataflow_graph","title":"create_dataflow_graph","text":"<pre><code>create_dataflow_graph(\n    spark: SparkSession,\n    default_catalog: Optional[str],\n    default_database: Optional[str],\n    sql_conf: Optional[Mapping[str, str]],\n) -&gt; str\n</code></pre> <p><code>create_dataflow_graph</code>...FIXME</p> <p><code>create_dataflow_graph</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"spark_connect_pipeline/#start_run","title":"start_run","text":"<pre><code>start_run(\n    spark: SparkSession,\n    dataflow_graph_id: str,\n    full_refresh: Optional[Sequence[str]],\n    full_refresh_all: bool,\n    refresh: Optional[Sequence[str]],\n    dry: bool,\n    storage: str,\n) -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p><code>start_run</code>...FIXME</p> <p><code>start_run</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"spark_connect_pipeline/#handle_pipeline_events","title":"handle_pipeline_events","text":"<pre><code>handle_pipeline_events(\n    iter: Iterator[Dict[str, Any]]\n) -&gt; None\n</code></pre> <p><code>handle_pipeline_events</code>...FIXME</p> <p><code>handle_pipeline_events</code> is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"sql/","title":"SQL","text":"<p>Spark Declarative Pipelines supports the following SQL statements to define data processing pipelines:</p> <ul> <li><code>CREATE FLOW AS INSERT INTO BY NAME</code> (Spark SQL)</li> <li><code>CREATE MATERIALIZED VIEW AS</code> (Spark SQL)</li> <li><code>CREATE STREAMING TABLE</code> (Spark SQL)</li> <li><code>CREATE STREAMING TABLE AS</code> (Spark SQL)</li> <li><code>CREATE VIEW</code> (Spark SQL)</li> <li><code>CREATE TEMPORARY VIEW</code> (Spark SQL)</li> <li><code>SET</code> (Spark SQL)</li> <li><code>SET CATALOG</code> (Spark SQL)</li> <li><code>USE NAMESPACE</code> (Spark SQL)</li> </ul> <p>Pipelines elements are defined in files with <code>.sql</code> file extension.</p> <p>The SQL files are included as <code>libraries</code> in a pipelines specification file.</p> <p>SqlGraphRegistrationContext is used on Spark Connect Server to handle SQL statements (from SQL definitions files and Python decorators).</p> <p>A streaming table can be defined without a query, as streaming tables' data can be backed by standalone flows. During a pipeline execution, it is validated that a streaming table has at least one standalone flow writing to the table, if no query is specified in the create statement itself.</p>"},{"location":"logical-operators/","title":"Logical Operators","text":"<p>Spark Declarative Pipelines extends Spark SQL with custom logical operators to support pipeline declarations in Python and SQL.</p>"},{"location":"logical-operators/CreateFlowCommand/","title":"CreateFlowCommand Binary Logical Operator","text":"<p><code>CreateFlowCommand</code> is a <code>BinaryCommand</code> logical operator that represents <code>CREATE FLOW ... AS INSERT INTO ... BY NAME</code> (Spark SQL) SQL statement in Spark Declarative Pipelines.</p> <p><code>CreateFlowCommand</code> is handled by SqlGraphRegistrationContext.</p> <p><code>Pipelines</code> execution planning strategy is used to prevent direct execution of Spark Declarative Pipelines SQL stataments.</p>"},{"location":"logical-operators/CreateFlowCommand/#creating-instance","title":"Creating Instance","text":"<p><code>CreateFlowCommand</code> takes the following to be created:</p> <ul> <li> Name (<code>UnresolvedIdentifier</code> leaf logical operator) <li> Flow operation (<code>InsertIntoStatement</code> (Spark SQL) unary logical operator) <li> Comment (optional) <p><code>CreateFlowCommand</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> (Spark SQL) is requested to parse <code>CREATE FLOW AS INSERT INTO BY NAME</code> SQL statement</li> </ul>"},{"location":"logical-operators/CreateMaterializedViewAsSelect/","title":"CreateMaterializedViewAsSelect Logical Operator","text":"<p><code>CreateMaterializedViewAsSelect</code> is a CreatePipelineDatasetAsSelect binary logical command that represents <code>CREATE MATERIALIZED VIEW ... AS</code> (Spark SQL) SQL statement in Spark Declarative Pipelines framework.</p> <p><code>CreateMaterializedViewAsSelect</code> is handled by SqlGraphRegistrationContext.</p>"},{"location":"logical-operators/CreateMaterializedViewAsSelect/#creating-instance","title":"Creating Instance","text":"<p><code>CreateMaterializedViewAsSelect</code> takes the following to be created:</p> <ul> <li> Name (<code>LogicalPlan</code> (Spark SQL)) <li> Columns (<code>ColumnDefinition</code>s) <li> Partitioning (<code>Transform</code> (Spark SQL)) <li> <code>TableSpecBase</code> <li> Query (<code>LogicalPlan</code> (Spark SQL)) <li> SQL Text <li> <code>ifNotExists</code> flag <p><code>CreateMaterializedViewAsSelect</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> (Spark SQL) is requested to parse <code>CREATE MATERIALIZED VIEW ... AS</code> SQL statement</li> </ul>"},{"location":"logical-operators/CreatePipelineDataset/","title":"CreatePipelineDataset Logical Commands","text":"<p><code>CreatePipelineDataset</code> is an extension of the <code>Command</code> (Spark SQL) abstraction for CREATE pipeline commands in Spark Declarative Pipelines framework:</p> <ul> <li><code>CREATE MATERIALIZED VIEW ... AS</code> (Spark SQL)</li> <li><code>CREATE STREAMING TABLE</code> (Spark SQL)</li> <li><code>CREATE STREAMING TABLE AS</code> (Spark SQL)</li> </ul>"},{"location":"logical-operators/CreatePipelineDataset/#contract-subset","title":"Contract (Subset)","text":""},{"location":"logical-operators/CreatePipelineDataset/#name","title":"Name","text":"<pre><code>name: LogicalPlan\n</code></pre>"},{"location":"logical-operators/CreatePipelineDataset/#implementations","title":"Implementations","text":"<ul> <li>CreatePipelineDatasetAsSelect</li> <li>CreateStreamingTable</li> </ul>"},{"location":"logical-operators/CreatePipelineDatasetAsSelect/","title":"CreatePipelineDatasetAsSelect Binary Logical Commands","text":"<p><code>CreatePipelineDatasetAsSelect</code> is an extension of <code>BinaryCommand</code> (Spark SQL) and CreatePipelineDataset abstractions for CTAS-like CREATE statements:</p> <ul> <li><code>CREATE MATERIALIZED VIEW AS</code> (Spark SQL)</li> <li><code>CREATE STREAMING TABLE AS</code> (Spark SQL)</li> </ul> <p><code>CreatePipelineDatasetAsSelect</code> is a <code>CTEInChildren</code>.</p>"},{"location":"logical-operators/CreatePipelineDatasetAsSelect/#contract","title":"Contract","text":""},{"location":"logical-operators/CreatePipelineDatasetAsSelect/#query","title":"Query","text":"<pre><code>query: LogicalPlan\n</code></pre>"},{"location":"logical-operators/CreatePipelineDatasetAsSelect/#originalText","title":"Original SQL Text","text":"<pre><code>originalText: String\n</code></pre>"},{"location":"logical-operators/CreatePipelineDatasetAsSelect/#implementations","title":"Implementations","text":"<ul> <li>CreateMaterializedViewAsSelect</li> <li>CreateStreamingTableAsSelect</li> </ul>"},{"location":"logical-operators/CreateStreamingTable/","title":"CreateStreamingTable Unary Logical Command","text":"<p><code>CreateStreamingTable</code> is a <code>UnaryCommand</code> (Spark SQL) and a CreatePipelineDataset that represents <code>CREATE STREAMING TABLE</code> (Spark SQL) SQL statement (with no <code>AS</code> clause) in Spark Declarative Pipelines framework.</p> <p><code>CreateStreamingTable</code> is handled by SqlGraphRegistrationContext.</p> CreateStreamingTableAsSelect for <code>CREATE STREAMING TABLE AS</code> SQL Statement <p><code>CREATE STREAMING TABLE AS</code> SQL statement (with <code>AS</code> clause) gives a CreateStreamingTableAsSelect binary logical command.</p>"},{"location":"logical-operators/CreateStreamingTable/#creating-instance","title":"Creating Instance","text":"<p><code>CreateStreamingTable</code> takes the following to be created:</p> <ul> <li> Name (<code>LogicalPlan</code> (Spark SQL)) <li> Columns (<code>ColumnDefinition</code>s) <li> Partitioning (<code>Transform</code>s (Spark SQL)) <li> <code>TableSpecBase</code> <li> <code>ifNotExists</code> flag <p><code>CreateStreamingTable</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> (Spark SQL) is requested to parse <code>CREATE STREAMING TABLE</code> SQL statement</li> </ul>"},{"location":"logical-operators/CreateStreamingTableAsSelect/","title":"CreateStreamingTableAsSelect Binary Logical Command","text":"<p><code>CreateStreamingTableAsSelect</code> is a CreatePipelineDatasetAsSelect binary logical command that represents <code>CREATE STREAMING TABLE AS</code> (Spark SQL) SQL statement in Spark Declarative Pipelines framework.</p> <p><code>CreateStreamingTableAsSelect</code> is handled by SqlGraphRegistrationContext.</p> CreateStreamingTable for <code>CREATE STREAMING TABLE</code> SQL Statement <p><code>CREATE STREAMING TABLE</code> SQL statement (with no <code>AS</code> clause) gives a CreateStreamingTable unary logical command.</p>"},{"location":"logical-operators/CreateStreamingTableAsSelect/#creating-instance","title":"Creating Instance","text":"<p><code>CreateStreamingTableAsSelect</code> takes the following to be created:</p> <ul> <li> Name (<code>LogicalPlan</code> (Spark SQL)) <li> Colums (<code>ColumnDefinition</code>s) <li> Partitioning (<code>Transform</code>s (Spark SQL)) <li> <code>TableSpecBase</code> <li> <code>CREATE</code> query (<code>LogicalPlan</code> (Spark SQL)) <li> Original SQL Text <li> <code>ifNotExists</code> flag <p><code>CreateStreamingTableAsSelect</code> is created when:</p> <ul> <li><code>SparkSqlAstBuilder</code> (Spark SQL) is requested to parse <code>CREATE STREAMING TABLE AS</code> SQL statement</li> </ul>"}]}